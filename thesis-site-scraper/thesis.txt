Title - Information systems and quality management systems: researching lifecycle synergies
Author - João Nuno Lopes Barata[https://estudogeral.sib.uc.pt/browse?type=author&value=Barata%2C+Jo%C3%A3o+Nuno+Lopes]
Advisor - Paulo Rupino da Cunha[https://estudogeral.sib.uc.pt/browse?type=author&value=Cunha%2C+Paulo+Rupino+da];
Keywords - Sistemas de Informação;Sistemas de Gestão da Qualidade;ISO 9001;ISO2;Cultura da Qualidade;Gestão de Processos de Negócio;Conformidade Regulamentar;Sinergias
Date - 1-Sep-2015
Abstract - Propomos uma abordagem para o desenvolvimento sinergístico do sistema de informação (SI) e do sistema de gestão da qualidade (SGQ), no contexto da ISO 9001. A ISO 9001 é uma norma da qualidade adotada por mais de um milhão de organizações em todo o mundo. A conformidade com normas e as regulamentações associadas é uma prioridade para as organizações certificadas pela ISO 9001. Estas necessitam de planear, documentar, auditar e melhorar continuamente os processos de negócio. Adicionalmente, o desenvolvimento do SGQ compreende a internalização dos princípios da qualidade na prática diária, envolvendo todos na promoção de uma cultura da qualidade. O sistema de informação é uma construção sociotécnica que usualmente inclui as tecnologias de informação (TI) para suportar os processos do negócio. Também as normas e regulamentação constituem uma fonte de informação para definir o contexto do SI. Neste cenário complexo, o ciclo de vida do desenvolvimento de SI (DSI) requer um desenho abrangente e suporte para a mudança, em conformidade com os objetivos, regras e políticas organizacionais. O SI e o SGQ são mutuamente dependentes, contudo, têm natureza diferente e a sua integração pode ser problemática. Especialmente se cada área vir a outra como uma mera forma de resolver as suas próprias necessidades: SI a disponibilizar ferramentas e informação para gestão da qualidade; SGQ a proporcionar orientação normativa com os princípios e práticas aplicáveis ao SI. Defendemos que uma terceira perspetiva é possível e desejável. A que combina os esforços ao longo do ciclo de desenvolvimento do SI/SGQ, abordando cinco dimensões interrelacionadas: (1) um contexto delineado por princípios da qualidade; (2) o envolvimento das pessoas no ciclo de desenvolvimento do SI/SGQ; (3) os processos de negócio formais e informais; (4) as TI formais e informais que os suportam; e (5) os fluxos de informação/dados. O principal contributo desta tese é uma abordagem que designámos por ISO2. Foi delineada a partir de uma revisão sistemática da literatura, entrevistas com auditores da qualidade e catorze estudos de caso. Posteriormente, validámos e refinámos a ISO2 em três projetos de investigação ação. Obtivemos evidências que a ISO2 pode: (1) proporcionar um guia passo-a-passo para assistir ao desenvolvimento sinergístico do SI/SGQ; (2) facilitar uma perceção comum acerca das interdependências entre os sistemas ao longo do ciclo de vida, pelos especialistas do SI/SGQ; (3) definir objetivos e regras para o desenvolvimento do SI/SGQ, considerando cinco dimensões principais: contexto, pessoas, processos, TI e informação/dados; (4) incluir a regulamentação no desenvolvimento conjunto do SI e do SGQ; (5) criar artefactos que interligam cada uma das cinco dimensões identificadas – artefactos O2; (6) aferir e melhorar a cultura da qualidade do SI; (7) aferir e melhorar a cultura da qualidade dos processos de negócio. De acordo com as organizações no nosso estudo, a ISO2 é simples de utilizar na fase de desenho, integrando os pontos de vista de várias partes interessadas do SI/SGQ. Adicionalmente, na fase de operação, ajuda na auditoria e melhoria do SI e do SGQ, sugerindo um plano de ação unificado que explora sinergias. A ISO2 pode dar um contributo relevante na transição para a nova versão da ISO 9001, esperada para 2015 e que antecipa (1) a importância crescente dos princípios da qualidade; (2) uma avaliação mais abrangente do contexto organizacional; (3) a necessidade de ter informação documentada; e (4) uma melhor evidência de melhoria. We propose an approach for the synergistic development of the information system (IS) and the quality management system (QMS), in the context of ISO 9001. ISO 9001 is a quality standard adopted by more than one million organizations worldwide. Compliance with standards and their regulations is a foremost concern for ISO 9001-certified organizations. They are asked to plan, document, audit, and continuously improve their business processes. Moreover, QMS development comprises the internalization of quality principles into daily practice, fostering a quality culture in the entire organization. An information system is a socio-technical construct that usually includes information technologies (IT) in support of the business processes. Additionally, standards and regulations become a vital source of information to define the context in which the IS operates. In this complex scenario, IS development (ISD) lifecycle calls for a comprehensive design and support for continuous change. It must ensure conformity to the organizational goals and rules, aligned with the organizational policies. The IS and the QMS depend on each other; however, they are different in nature and their integration can be problematic. Especially, if each field sees the other as a mere way to solve their own needs: the IS in providing tools and information for quality management; the QMS in providing normative guidance with principles and practices that apply to the IS. We argue that a third perspective is possible and desirable. The one which combines efforts within the IS/QMS development lifecycle, addressing five interrelated dimensions: (1) a context that is shaped by quality principles; (2) the people involvement in the IS/QMS development lifecycle; (3) the formal and informal business processes; (4) the formal and informal IT that supports them; and (5) the flows of information/data. The main outcome of this thesis is an approach that we named ISO2. It was first drafted from a systematic literature review, interviews with quality auditors, and fourteen case studies. We then validated and refined ISO2 in three action research projects. We gathered evidence in our research that ISO2 can: (1) provide a step-by-step guide to assist the synergistic development of the IS and QMS; (2) facilitate a common understanding by the IS/QMS experts about the interdependence between their systems within the lifecycle; (3) define goals and rules for the systems development, considering five main dimensions: context, people, process, IT, and information/data; (4) integrate regulations in the joint development of the IS and the QMS; (5) create artifacts that link each of the five main dimensions identified – the O2 artifacts; (6) assess and improve IS quality culture; (7) assess and improve the business processes quality culture. According to the organizations in our study, the ISO2 approach is simple to use at design-time, integrating the viewpoints of IS/QMS stakeholders. Moreover, at run-time, it backs the audit and improvement of the IS and the QMS, suggesting a unified action plan that explores synergies. ISO2 can also offer a key contribution in the transition to the new version of ISO 9001, to be released in 2015, which anticipates (1) the increasing importance of quality principles; (2) a broader evaluation of organizational context; (3) the need of documented information; and (4) advanced evidence of improvement.
Thesis Type - Doctoral Thesis
URI - http://hdl.handle.net/10316/28384
Title - Enabling Wireless Cooperation in User Provided Networks
Author - Vitor Guerra Rolla[https://estudogeral.sib.uc.pt/browse?type=author&value=Rolla%2C+Vitor+Guerra]
Advisor - Marilia Curado[https://estudogeral.sib.uc.pt/browse?type=author&value=Curado%2C+Marilia];
Keywords - Delay tolerant routing;User provided networks;Incentive mechanism
Date - 19-Jun-2015
Abstract - This doctoral thesis investigates user provided networks. Such networks have become important research subjects in the field of informatics engineering due to the recent popularity of smart phones. User provided networks are independent from traditional Internet service providers. Communication and information exchange between users occurs opportunistically, i.e., when the smart phones are close enough to exchange information. Most user provided networks are based on the radio standard IEEE 802.11, popularly known as 'wi-fi'. However, some networks are based on other low range radio standards, such as Bluetooth and IEEE 802.15.4. User provided networks are important to the society in general when the traditional Internet service providers become unavailable. For example, this may occur in terrorist attacks, earthquakes, or even cyber attacks. In these emergency situations, when users have a greater interest in common, an efficient system for non-presencial information exchange is necessary. Such networks are also interesting in a social context, when users must be incentivized to share their resources (storage capacity, wireless connectivity and battery) to enable the exchange of information. This doctoral thesis addresses both situations: i) networks whose users have a common interest and ii) networks whose users need to be encouraged to share resources. Among the various contributions of this doctoral thesis are the Delay Tolerant Reinforcement-Based routing solution and the Messages on oFfer incentive mechanism. The first is a routing solution for users-provided networks when the users have a prior interest in common. The second is an incentive mechanism to encourage users to exchange information. Both solutions showed excellent results in the simulation environment. Esta tese de doutoramento investiga as redes providas pelos usuários. Com a popularização do telemóvel esperto (smart phone) tais redes se tornaram objeto de pesquisa na área de engenharia informática. Uma característica básica das redes providas pelo usuário é a sua independência em relação aos provedores de serviço tradicionais. A comunicação e troca de informação entre usuários ocorre de forma oportuna, isto é, quando os telemóveis estão próximos o suficiente para se comunicarem. A maioria das redes providas por usuários é baseada no padrão de rádio IEEE 802.11, popularmente conhecido como 'wi-fi'. No entanto, algumas redes se baseiam em outros padrões de baixo alcance, por exemplo Bluetooth e IEEE 802.15.4. As redes providas por usuários são importantes para a sociedade no advento dos provedores de serviço tradicionais ficarem indisponíveis. Por exemplo, isso pode ocorrer em ataques terroristas, terremotos, ou mesmo em ataques virtuais. Nessas situações de emergência, quando os usuários têm um interesse maior em comum, é necessário um sistema de troca de mensagens não presencial eficiente. Tais redes também são importantes em um contexto social, quando os usuários precisam ser incentivados a compartilhar os seus recursos (capacidade de armazenamento, conectividade sem-fio e bateria) para que ocorra troca de informação. Essa tese de doutoramento aborda ambas as situações: i) redes cujos usuários têm um interesse em comum e ii) redes cujos usuários precisam ser incentivados a compartilhar recursos. Dentro das diversas contribuições que esta tese de doutoramento apresenta estão a solução de roteamento Delay Tolerant Reinforcement-Based e o mecanismo de incentivo Messages on oFfer. A primeira é uma solução de encaminhamento para redes providas por usuários que tenham um interesse em comum prévio. A segunda é um mecanismo de incentivo para estimular que os usuários troquem informação quando não houver o interesse em comum. Ambas as soluções apresentaram excelentes resultados no ambiente de simulação desenvolvido nesta tese.
Thesis Type - Doctoral Thesis
URI - http://hdl.handle.net/10316/27200
Title - A Educação de uma Nova Geração de Jornalistas: Do Moodle ao Facebook
Author - Carla Susana Ribeiro Patrão[https://estudogeral.sib.uc.pt/browse?type=author&value=Patr%C3%A3o%2C+Carla+Susana+Ribeiro]
Advisor - António Dias Figueiredo[https://estudogeral.sib.uc.pt/browse?type=author&value=Figueiredo%2C+Ant%C3%B3nio+Dias];
Keywords -  
Date - 17-Mar-2015
Abstract - A educação superior para o jornalismo é um fenómeno relativamente recente na sociedade portuguesa, no âmbito do qual ainda se abrem oportunidades de investigação relevantes. Numa altura em que o jornalismo é atravessado por questões que incidem sobre a sua sustentabilidade, função e direccionamento sociais, temos assistido à transformação de uma profissão que cada vez mais adquire características de precariedade e fragilidade. A educação superior para o jornalismo, neste contexto, é chamada a assumir um novo enquadramento das suas práticas, nomeadamente através da tradução e concretização de elementos facilitadores da integração num percurso profissional que certamente pertence às expectativas dos que a ela recorrem. Esta tese concretiza um contributo exploratório para a compreensão e realização desse desafio, abordando o problema de como educar a nova geração de jornalistas através de experiências de aprendizagem inovadoras baseadas em contextos sociais de aprendizagem mediados pelas novas tecnologias. Mais concretamente, procuraram-se definir as sinergias de uma estratégia pedagógica pela via da reflexão pessoal, fundamentada numa análise fenomenológica das perspectivas dos alunos e das experiências realizadas num contexto real. Nesta reflexão foi central o pensamento de autores como Dewey, Freire e McLuhan, bem como elaborações teóricas sobre actividade humana enquanto fenómeno social e cultural. Considerando o carácter exploratório da investigação, seguiu-se uma abordagem metodológica de investigação-acção em três ciclos, cada um deles alicerçado nos resultados do anterior, tendo privilegiado, dessa forma, tanto aspectos cumulativos como diferenciadas. As experiências preliminares na plataforma Moodle salientaram a amplificação do contexto de aprendizagem resultante da estratégia pedagógica baseada em projecto de publicação, avaliação entre pares e discussão temática, para uma experiência eminentemente social, liberta dos constrangimentos espaciais e temporais da sala de aula. No segundo ciclo testou-se a mesma estratégia pedagógica, na plataforma de gestão de comunidades Dolphin, tendo em vista uma primeira definição da construção social e cultural da experiência de aprendizagem nas perspectivas dos alunos, atentando também na exploração do potencial de desenvolvimento de uma comunidade. Deste movimento ficou visível, nos relatos dos alunos, a aproximação da experiência às expectativas de um futuro profissional e as vantagens da Internet para uma promoção pessoal por via da exposição de trabalhos jornalísticos. Por outro lado, a comunidade desejada falhou diante da ausência dos atractivos da exposição externa e da conclusão das actividades curriculares formais. Um terceiro ciclo de investigação-acção testou uma combinação entre um blogue de tipo portefólio colectivo e um grupo aberto na rede social Facebook. Este teste à estratégia pedagógica foi novamente descrito pelos alunos como um contexto de simulação profissional, associado a sentimentos e representações que pertencem a uma esfera da profissão de jornalista, contexto esse impulsionado claramente pela maior exposição externa permitida pelo Facebook, sem prejuízo do suporte tecnológico às interacções sociais, e aproveitando da melhor forma as novas oportunidades para a prática do jornalismo permitidas no ciberespaço. Os resultados observados nestes ciclos de investigação-acção contribuem para o argumento central demonstrado nesta tese, de como uma estratégia pedagógica apoiada numa relação dinâmica e interdependente entre duas dimensões resultantes do âmbito da actividade, nomeadamente participação e exposição, resultou em sinergias observadas nas representações dos alunos sobre a experiência de aprendizagem que se situam no âmbito da competência prática em jornalismo e da consciencialização pessoal sobre um percurso até à profissão. Higher education in the field of journalism is a relatively recent phenomenon in Portuguese society and is consequently still open to relevant research perspectives. At a time when journalism is subject to questions regarding its sustainability, function, and social purpose, access to the profession has become increasingly narrow and precarious. In this context, a higher education in journalism is called upon to assume new perspectives on its practices, namely through the translation and concretization of elements that facilitate an integration into a professional trajectory, which certainly corresponds to the expectations of those who seek out higher education in this field. This thesis presents an exploratory contribution to the understanding and manifestation of this challenge, by tackling the problem of how to educate a new generation of journalists through innovative learning experiences based on social contexts of learning via new technologies. More specifically, this thesis searches to define the synergies of a pedagogic strategy through personal reflection, based on a phenomenological analysis of student perspectives and their experiences acquired in a real context. The perspectives of authors like Dewey, Freire, and McLuhan were central to this reflection as well as other theoretical approaches to human activity as a social and cultural phenomenon. In view of the exploratory approach of this investigation, a methodological approach was followed in three cycles of action research, each one of them based on the results of the one prior, focusing therefore both on cumulative and distinct features. The preliminary experiments conducted using the platform Moodle highlighted the amplification of the learning context as a result of a pedagogic strategy based on a publishing project, peer evaluation, and thematic discussion, lending itself to a highly social experience liberated from the time and space constraints of the classroom. In the second cycle, the same pedagogic strategy was tested with the objective of creating a firsthand definition of social and cultural learning experience from the students’ perspectives, while also aiming to explore the potential development of a community within a Dolphin platform. According to the students’ reports, this movement demonstrated that their experiences came close to matching their expectations of what a professional future will hold, as well as the advantages of the Internet for personal promotion via the exposure of their journalistic work. However, the desired Dolphin community failed when faced with an absence of the benefits of external exposure and the end of formal curricular activity. A third cycle of action research tested a combination of a collective portfolio-type blog and an open group on the Facebook social network. This testing of pedagogic strategy was again described by the students as a context for professional simulation, associated with sentiments and representations that belong to the sphere of professional journalism. This context was clearly driven by better external exposure made possible by Facebook. Similar to the previous cycles, this social platform also allowed the social interactions, but represented a leap forward in capitalizing on the new opportunities in the practice of journalism offered in cyberspace. The observed results of these action research cycles contributed to the central argument of this thesis: how a pedagogic strategy supported by a dynamic and interdependent relationship between the two dimensions resulting from the scope of activity – participation and exposure – resulted in the observed synergies in students’ representations of the learning experience, about the learning of professional skills in journalism and the acquisition of a state of conscience, in regards to a personal path into the profession
Thesis Type - Doctoral Thesis
URI - http://hdl.handle.net/10316/27151
Title - Massively Scalable Data Warehouses with Performance Predictability
Author - João Pedro Matos da Costa[https://estudogeral.sib.uc.pt/browse?type=author&value=Costa%2C+Jo%C3%A3o+Pedro+Matos+da]
Advisor - Pedro Furtado[https://estudogeral.sib.uc.pt/browse?type=author&value=Furtado%2C+Pedro];
Keywords - Data Warehouse
Date - 11-Jun-2015
Abstract - Data Warehouses (DW) são ferramentas fundamentais no apoio ao processo de tomada de decisão, e que lidam com grandes volumes de dados cada vez maiores, que normalmente são armazenados usando o modelo em estrela (star schema). No entanto, o resultado das pesquisas e análises deve estar disponível em tempo útil. Contudo, como a complexidade das pesquisas que são submetidas é cada vez maior, com padrões de pesquisa imprevisíveis (ad-hoc), e devido ao aumento do número de pesquisas que são submetidas e executadas simultaneamente, provoca que o tempo de execução das pesquisas seja imprevisível. Mercados concorrenciais requerem que os resultados sejam disponibilizados em tempo útil para ajudar o processo de tomada de decisão. Isto não é apenas uma questão de obter resultados rápidos, mas de garantir que os resultados estarão disponíveis antes das decisões serem tomadas. Estratégias de pré-computação de pesquisas podem ajudar na obtenção de resultados mais rápidos, no entanto a sua utilização é limitada apenas a pesquisas com padrões conhecidos (planeados). Contudo, as consultas com padrões de pesquisa imprevisíveis (ad-hoc) são executadas sem quaisquer garantias de execução de tempo. São vários os fatores que influenciam a capacidade da DW fornecer resultados às pesquisas em tempo útil, tais como a complexidade da pesquisa (seletividade, número de tabelas que necessitam ser relacionadas, os algoritmos de junção e o tamanho das tabelas), a heterogeneidade e a capacidade da infraestrutura de processamento, incluindo a velocidade de leitura de disco, e à memória disponível para efetuar a junção das tabelas. O aumento do volume de dados e do número de pesquisas que estão a ser simultaneamente executadas também influenciam a capacidade do sistema em fornecer tempos de execução previsíveis. Apesar do tempo e esforço despendido para definir infraestruturas de processamento paralelo com capacidade para lidar com o aumento do volume de dados, e melhorar o tempo de execução das pesquisas, estas não permitem garantir a disponibilização atempada dos resultados, particularmente para as pesquisas ad-hoc. O tempo de execução de pesquisas com padrões conhecidos pode ser otimizado através de um conjunto de estratégias e mecanismos auxiliares, tais como a utilização de vistas materializadas e indexes. No entanto, para consultas ad-hoc, tais mecanismos não são uma solução. A imprevisibilidade do padrão de pesquisas origina tempos de execução imprevisíveis, que podem ser incompatíveis com os requisitos de negócio. Além disso, para muitos negócios, o crescente volume de dados condiciona ainda mais a capacidade da infraestrutura de processamento de fornecer resultados em tempo útil. Como consequência, os departamentos de TI estão constantemente atualizando a infraestrutura de processamento com a espectativa de que esta seja capaz de processar atempadamente as pesquisas, mas sem nenhuma garantia de que o consiga fazer. Não existe um método concreto que permita definir os requisitos mínimos de hardware que permita a execução atempada das pesquisas. Esta dissertação propõe uma arquitetura de Data Warehouse escalável com capacidade de lidar com grandes volumes de dados e de fornecer resultados em tempo útil, mesmo quando um grande número de pesquisas estão a ser simultaneamente executadas. A capacidade de fornecer resultados em tempo útil não é apenas uma questão de desempenho, mas uma questão de ser capaz de retornar atempadamente os resultados às pesquisas, quando esperado, de acordo com a natureza da análise e das decisões do negócio. O conceito de execução atempada (obtenção de resultados em tempo útil) é introduzido, e são propostos mecanismos que permitem fornecer garantias de execução atempada, sem no entanto descurar os requisitos de previsibilidade do tempo de execução das pesquisas e de latência mínima (frescura dos dados - freshness). A complexidade da execução de uma pesquisa é influenciada por diversos fatores, tais como a seletividade da pesquisa, o tamanho das tabelas, o número de junções e os algoritmos de junção. O volume de dados e memória disponível para junções, influenciam tanto a ordem de junção bem como o algoritmo de junção utilizado, resultando em custos de execução imprevisíveis. A necessidade de juntar as tabelas de dimensão com a tabela de factos advém do modelo em estrela (star-schema). O volume de dados é outro fator de imprevisibilidade, não sendo possível determinar com precisão o impacto do aumento do volume de dados no tempo de execução das pesquisas. Para lidar com estes fatores de imprevisibilidade relacionados com a junção de tabelas, propusemos o modelo de dados desnormalizado, chamado ONE. Neste modelo, os dados da tabela de factos, assim como os correspondentes dados das tabelas de dimensão, são fisicamente guardados numa única tabela desnormalizada, contendo todos os atributos das tabelas. O modelo de dados ONE requer mais espaço para guardar os dados, no entanto o modelo de processamento é mais simples e com tempos de execução previsíveis. Com o modelo de dados ONE, a tabela desnormalizada é particionada em fragmentos de dados mais pequenos e distribuídos pelos nós da infraestrutura para processamento paralelo, obtendo-se um aumento de desempenho. ONE possibilita uma escalabilidade quase ilimitada, uma vez que a totalidade dos dados (dos factos e das dimensões), e não apenas da tabela de factos, é linearmente dividida pelos nós da infraestrutura de processamento (com η nós homogéneos, cada nó conterá 1/η dos dados). Portanto, e uma vez que a adição de novos nós à infraestrutura de processamento não requer a replicação das dimensões, o modelo ONE oferece escalabilidade massiva de dados. Ao garantir uma distribuição linear de todos os dados, e não apenas os dados da tabela de fatos, o tempo de execução das pesquisas é melhorado proporcionalmente à redução do volume de dados em cada nó. Além disso, e porque os dados estão desnormalizados, o processamento das pesquisas é bastante simplificado e previsível, pois fica reduzido às operações de filtragem e de agregação dos dados. Como consequência, são reduzidos os requisitos da infraestrutura de processamento. Por norma, quando uma pesquisa é submetida não existe uma noção clara de quanto tempo irá demorar e se o resultado será obtido antes da tomada de decisão. Definimos o conceito de execução em tempo útil (right-time) como a capacidade de executar pesquisas de modo que os resultados estejam disponíveis antes da tomada de decisão (execução atempada), antes dum determinado objetivo temporal. O objetivo não é obter execuções mais rápidas, mas sim garantir que os resultados estarão disponíveis quando esperado. São propostos mecanismos que permitem fornecer previsibilidade de tempo de execução e garantias de execução atempada de pesquisas que tenham objetivos temporais. Como as pesquisas podem ter objetivos temporais diferentes do oferecido pela atual infraestrutura de processamento, propusemos um modelo de processamento chamado TEEPA (Timely Execution with Elastic Parallel Architecture), que toma em consideração os objetivos temporais das pesquisas para ajustar e rebalancear a infraestrutura de processamento de modo a que estes sejam garantidos. Quando a infraestrutura atual não consegue executar atempadamente as pesquisas, são adicionados mais nós de processamento e o volume de dados é redistribuído entre eles. Em cada nó, TEEPA monitora continuamente a execução da pesquisa, o volume de dados alocado, e a taxa de transferência IO, para determinar se as pesquisas podem ser atempadamente executadas. Como os nós de processamento podem ser heterogéneos, TEEPA toma em conta as suas capacidades de IO para determinar quantos nós são necessários e como deve ser efetuada a redistribuição dos dados. O volume de dados alocado em cada nó é ajustado em função do volume total (número total de registos), do tamanho do registo e da taxa de transferência de cada nó. Deste modo, a nós mais rápidos são atribuídos maiores volumes de dados. O processo de seleção e integração de novos nós de processamento e posterior rebalanceamento e reequilíbrio dos dados é executado até que os objetivos temporais sejam atingidos. Por outro lado, cada vez mais há a necessidade de analisar dados obtidos quase em tempo real, com mínima latência e frescura (freshness), o que requer que os dados sejam carregados mais frequentemente, à medida que são registados. Contudo, tipicamente as DW são refrescadas periodicamente com conjuntos de registos (batch), de modo a reduzir os custos de carregamento e os custos relacionados com o refrescamento de estruturas auxiliares, como índices e vistas materializadas. Sistemas de base de dados em memória minimizam estes custos, e possibilitam que os dados sejam carregados mais frequentemente. Contudo, a memória é finita e é insuficiente para conter a totalidades dos dados. De modo a oferecer latência mínima, definimos um modelo de processamento paralelo em que os dados são divididos em duas partes distintas: os dados antigos são guardados no modelo de dados ONE, ao qual chamámos Od, e os dados mais recentes são guardados em memória num modelo em estrela, designado de Os. Os dados podem ser carregados com maior frequência para Os, reduzindo assim a sua latência, e são aí mantidos enquanto existir memória disponível. Quando for necessário, por exemplo quando for necessário libertar memória para guardar novos dados, os dados mais antigos existentes em Os são movidos para Od. A utilização dum modelo hibrido, composto por Od e Os, permite que as DW existentes, que utilizam o modelo em estrela, possam ser migradas diretamente para este modelo com mínimo impacto ao nível dos processos de extração, transformação e carregamento dos dados (ETL). Na perspetiva do utilizador e das aplicações, este modelo hibrido oferece uma visão lógica dos dados num modelo em estrela, por forma a permitir uma fácil integração com aplicações e processos de carregamentos existentes, e a oferecer as vantagens do modelo em estrela, nomeadamente ao nível de usabilidade e facilidade de utilização. Uma camada de abstração gere a consistência de dados e processamento entre as duas componentes (Os e Od), incluindo a reescrita das pesquisas de modo a processar os dados que se encontram em cada uma das componentes. São também propostos mecanismos que oferecem garantias de execução atempada de pesquisas, mesmo quando um grande número de pesquisas está sendo processado simultaneamente. Infraestruturas paralelas podem minimizar esta questão, no entanto a sua escalabilidade é limitada pelo modelo de execução dos sistemas de bases de dados relacionais, onde cada pesquisa é processada individualmente e compete com as outras pelos recursos (IO, CPU, memória, …). É proposto um modelo de processamento de pesquisas, chamado SPIN, que analisa as pesquisas submetidas e, sempre que possível, efetua a partilha de dados e processamento entre elas, e assim consegue oferecer tempos de execução mais rápidos e previsíveis. SPIN utiliza o modelo de dados ONE, mas considera a tabela como sendo circular, isto é, uma tabela que é lida continuamente de uma forma circular. Enquanto existirem pesquisas a serem executadas, os dados são lidos sequencialmente e quando chega ao fim da tabela, recomeça a ler os dados desde o início da tabela. À medida que os dados são lidos, estes são colocados sequencialmente numa janela deslizante em memória (base pipeline), para serem partilhados pelas várias pesquisas. Cada pesquisa processa todos os registos da tabela, no entanto a leitura e o processamento não começa no registo número 1 da tabela, mas sim no primeiro registo da janela deslizante (início lógico). Os restantes registos são processados à medida que forem lidos e colocados na janela deslizante, até que o próximo registo a ser processado seja o do início lógico, isto é, após um ciclo completo. O custo da leitura dos dados é constante e partilhado por todas as pesquisas. Deste modo, a submissão de novas pesquisas não introduz custos adicionais ao nível da leitura de dados. O tempo de execução das pesquisas é influenciado apenas pela complexidade e número dos filtros (restrições) das pesquisas e pelo custo das agregações e ordenações dos dados. SPIN partilha dados e processamento entre pesquisas, combinando filtros e computações comuns a várias pesquisas num único fluxo (ramo) de processamento. Os vários ramos (branches) são sequencialmente conectados, formando uma estrutura em árvore que denominámos de WPtree (Workload Processing Tree), que tem como raiz o base pipeline. Quando uma pesquisa é submetida, se existir um ramo de processamento com predicados comuns aos da pesquisa, a pesquisa é encadeada como um novo ramo desse ramo comum, e são removidos os respetivos predicados da pesquisa. Se não existir um ramo com predicados comuns, a pesquisa é encadeada como um novo ramo do base pipeline. Deste modo, reduz-se o volume de dados que está em memória para processamento, bem como o custo de processamento dos predicados. A árvore de processamento é continuamente monitorizada, e quando necessário, um optimizador reorganiza dinamicamente o número e a ordem dos ramos. Sempre que possível, uma pesquisa é processada através da combinação dos resultados que estão a ser processados por outros ramos, e deste modo simplificando e reduzindo o volume de dados que a pesquisa tem que processar. Como os registos são lidos e processados pela mesma ordem, enquanto os dados não forem alterados, o resultado da avaliação dos predicados de cada registo é o igual ao da última vez que foi avaliado. De modo a evitar o custo da avaliação de registos anteriormente avaliados, e que não foram alterados, é proposta uma extensão ao modelo de processamento SPIN que utiliza uma abordagem de processamento baseada em bitsets (estruturas similares aos índices bitmaps). Um bitset é construído para cada ramo com o resultado da avaliação dos seus predicados, sendo o resultado de cada registo guardado na correspondente posição do bitset. Após o bitset estar completo, a posterior avaliação desses predicados pode ser substituído por uma simples verificação no bitset. Os bitsets têm um tamanho reduzido e são guardados em memória, de modo a evitar a introdução de custos adicionais ao nível de IO. Bitsets são particularmente relevantes para predicados complexos e com elevado custo de processamento, sendo criados e removidos dinamicamente de acordo com uma política de retenção, que toma em consideração vários aspetos, tais como a memória disponível, cardinalidade, e o custo da avaliação dos predicados. Através da análise do conjunto sequencial de ramos (path) de uma pesquisa, e dos custos de processamento de cada ramo, é possível estimar, com elevada precisão, o tempo de execução da pesquisa, mesmo quando existe um grande número de pesquisas a serem executadas simultaneamente. Para satisfazer pesquisas com objetivos temporais mais exigentes, é proposto um mecanismo de processamento, denominado CARROUSEL, que além de redistribuir e/ou replicar fragmentos dos dados pelos vários nós de processamento, redistribui também o processamento das pesquisas e dos ramos pelos nós. Tomando em consideração os bitsets existentes, é possível determinar quais os fragmentos de dados que cada pesquisa necessita processar e deste modo reduzir o custo de processamento através da ativação/desativação dinâmica dos ramos, consoante os fragmentos que estão nesse momento em memória. É possível terminar antecipadamente a execução de uma pesquisa, antes do término do ciclo. CARROUSEL é um processador flexível de fragmentos que utiliza um conjunto de nós inativos, ou nós que estão a executar pesquisas com objetivos temporais menos exigentes, para processar em paralelo alguns dos fragmentos de dados requeridos por pesquisas com objetivos temporais mais exigentes. Ao reduzir-se o volume de dados a ser processado por cada nó consegue-se tempos de execução mais rápidos. Alternativamente, alguns dos ramos de processamentos podem ser redistribuídos para outros nós com réplicas dos fragmentos. A execução da pesquisa termina quando todos os registos foram processados. No entanto, como os dados estão continuamente a ser lidos e à medida que são processados é recolhida informação relevante sobre os dados que existem em cada fragmento. Esta informação é relevante de modo a decidir como o balanceamento dos fragmentos e dos ramos a processar devem ser redistribuídos pelos nós por forma a reduzir custos de processamento e tempos de execução. Data Warehouse (DW) systems are a fundamental tool for the decision-making process, have to deal with increasingly large data volumes, which is typically stored in as a star-schema model. The query workload is also more demanding, involving more complex, ad-hoc and unpredictable query patterns, with more simultaneous queries being submitted and executed concurrently. Modern competitive markets require decisions to be taken in a timely fashion. It is not just a matter of delivering fast analysis, but also of guaranteeing that they will be available before business-decisions are made. Moreover, the data volumes produced by data intensive industries are continuously increasing, stressing the processing infrastructure ability to provide such timely requirements even further. As a consequence, IT departments are continuously upgrading the processing infrastructure with the objective to hopefully the newer architecture will be able to deliver query results within the required time frame, but without any guarantees that it will be able to do so. There’s no concrete method to define the minimal hardware requirements to deliver timely query results. Several factors influence the ability of the DW infrastructure to provide timely results to queries, such as the query execution complexity (query selectivity, number of relations that have to be joined, the joins algorithms and the relations’ size), the heterogeneity and capabilities of the processing infrastructure, including IO throughput, and the memory available to process joins and the implementation of the join algorithms). Larger data volumes and concurrent query loads; concurrent queries that are executing simultaneously also influence the system ability to provide predictable execution times. In spite of all the time and effort to come up with a parallel infrastructure to handle such increase in data volume and to improve query execution time, it may be insufficient to provide timely execution queries, particularly for ad-hoc queries. The performance of well-known queries can be tuned through a set of auxiliary strategies and mechanisms, such as materialized views and index tuning. However, for ad-hoc queries, such mechanisms are not an alternative solution. The query patterns unpredictability result in unpredictable query execution times, which may be incompatible with business requirements. Data volumes produced by data intensive industries are continuously increasing, stressing the ability of the processing infrastructure to provide such timely answers even further. As a consequence, IT departments are continuously upgrading the processing infrastructure with the objective to deliver query results within the required time frame, but without any guarantees that it will be able to do so. There’s no concrete method to define the minimal hardware requirements to deliver timely query results. This dissertation proposes a data warehousing architecture that provides scalability and timely results for massive data volumes. The architecture is able to do this even in the presence of a large number of concurrent queries, and it is able to meet near real-time requirements. The ability to provide timely results is not just a performance issue (high throughput), but also a matter of returning query results when expected, according to the nature of the analysis and the business decisions. Query execution complexity is highly influenced by the number of relations that have to be joined together, the relations’ size and the query selection predicates (selectivity), influencing the data volume that has to be read from storage and joined. This data volume and the memory available for joins, influence both the join order and the used join algorithms. These unpredictable costs related to joining the fact table with dimensions relations arise from the star-schema model organization. The data volume is another factor of unpredictability, since there’s no simple and accurate method to determine the impact of larger data volumes in query execution time. To handle the unpredictability factors related to joining relations, we proposed the ONE data model, where the fact table and data from corresponding dimensions are physically stored into a single de-normalized relation, without primary and foreign keys, containing all the attributes from both fact and dimension tables. ONE trades-off storage space for a more simpler and predictable processing model. To provide horizontal scalability, we partitioned the de-normalized ONE relation into data fragments and distribute them among a set of processing nodes for parallel processing, yielding improved performance speedup. ONE delivers unlimited data scalability, since the whole data (fact and dimensions), and not just the fact table, is linearly partitioned among nodes (with η nodes, each will have 1/η of the ONE node). Therefore, since the addition of more nodes to the processing infrastructure does not require additional data replication of dimensions, ONE provides massive data scalability. By ensuring a linear distribution of the whole data, and not just the fact table, query execution time is improved proportionally to the data volume in each node. Moreover, since data in each node is already joined and thus query processing does not involve the execution of costly join algorithms, the speedup in each node is enhanced (almost) linearly as a function of the data volume that it has to process. By de-normalizing the data, we also decrease the nodes’ requirements, in what concerns physical memory (needed for processing joins), and query processing tasks, since the join processing tasks that were repeatedly (over and over) processed are removed. The remaining tasks, such as filtering and aggregations, have minimum memory and processing requirements. Only group by aggregations and sorting have memory requirements. The concept of timely results (right-time execution) is introduced, and we propose mechanisms to provide right-time guarantees while meeting runtime predictability and freshness requirements. The ability to provide right-time data analysis is gaining increasing importance, with more and more operational decisions being made using data analysis from the DW. The predictability of the query execution tasks is particularly relevant for providing right-time or real-time data analysis. We define right-time as the ability to deliver query results in a timely manner, before they are required. The aim is not to provide the fastest answers, but to guarantee that the answers will be available when expected and needed. We proposed a Timely Execution with Elastic Parallel Architecture (TEEPA) which takes into consideration the query time targets to adjust and rebalancing the processing infrastructure and thus providing right-time guarantees. When the current deployment is unable to deliver the time targets, it adds more processing nodes and redistributes the data volumes among them. TEEPA continuously monitors the local query execution, the IO throughput and the data volume allocated to each processing node, to determine if the system is able to satisfy the user specified time targets. TEEPA was designed to handle heterogeneous nodes and thus it takes into account their IO capabilities when performing the necessary data rebalancing tasks. The data volume allocated to each node is adjusted as a function of the whole data load (total number of tuples), the tuple size and the node’ sequential scan throughput, with larger data volumes allocated to faster processing nodes. The node allocation (selection and integration of newer nodes) and data rebalancing tasks are continuously executed until the time targets can be assured. There’s an increasing demand for data analyses over near real-time data, with low latency and minimum freshness, which requires data to be loaded more frequently or loaded in a row-by-row fashion. However, traditionally DWs are periodically refreshed in batches, to reduce IO loading costs and costs related to the refreshing indexes and pre-computed aggregation data structures. Main memory DBMS eliminate IO costs and thus can handle higher data loading frequencies. However, physical memory is limited in size and cannot typically hold the whole tables and structures. To provide freshness guarantees, the proposed architecture combines a parallel ONE deployment with an in-memory star-schema model holding recent data. The in-memory part (Os) maintains the recently loaded data, to allow the execution of real-time analyses. By using a star-schema model in Os, existing DW applications can be easily replaced and integrated with the architecture without the need to recreate the existing ETL tasks. Data is loaded into the in-memory Os and remains there for real-time processing while there’s memory available, so that the most recent data is held in the star-schema. When the physical memory is exhausted, the data in Os stored in the star-schema model is moved to Od in the ONE data model. From the user perspective and data presentation, the architecture offers a logical star-schema model view of the data, in order to provide easy integration with existing applications and because the model has advantages in what concerns users understanding and usability. A logical to physical layer manages data and processing consistency between models, including the necessary query rewriting for querying the data stored in each part, and merging of results. Finally we present the mechanisms of the architecture that allow it to still guarantee right-time execution in the presence of huge concurrent query loads. Modern DWs also suffers from workload scalability limitations, with more and more queries (in particular ad-hoc) being concurrently submitted. Larger parallel infrastructures can reduce this limitation, but its scalability is constrained by the query-at-time execution model of custom RDBMs, where each query is individually processed, competing for resources (IO, CPU, memory,…) and accessing the common base data, without data and processing sharing considerations. We propose SPIN, a data and processing sharing model that delivers predictable execution times for concurrent queries and overcomes the memory and scalability limitations of existing approaches. SPIN views the ONE relation in a node, as a logical circular relation, i.e. a relation that is constantly scanned in a circular fashion. When the end is reached, it continues scanning from the beginning, while there are queries running. Each query process all the required tuples of relation ONE, but the scanning and the query processing does not starts from the same first physical row. As the relation is read in a circular fashion, the first logical row is the one that already is cached in memory. The remaining tuples of the query are processed as they are being read from storage until the first logical row is reached. Data is read from storage and placed into an in–memory pipeline to be shared by all running concurrent queries. IO reading cost is constant and is shared between running queries. Therefore, the submission of additional queries does not incur in additional IO costs and joins operations. The execution times of concurrent queries are influenced by the number and complexity of the query constraints (filtering) and the cost of aggregations. To provide massive workload scalability it shares data and processing among queries, by combining the running queries in logical query branches for filtering clauses and by extensive reuse of common computations and aggregation operations. It analyses the query predicates, and if exists a logical branch in the current workload processing tree with common predicates it is registered in that logical branch, and the corresponding query predicates are removed. Otherwise, if do not exists a logical branch that meet the query predicates, it is registered as a new logical branch of the base data pipeline. This enhances processing sharing, and reduces the number of filtering conditions. The architecture has a branch optimizer that is continuously adjusting the number and order of the existing branches, and reorganizing them as required. Whenever possible, a query can merge and combine the results that are being processed by other branches, and thus simplifying and reducing the data volume that the query branch has to filter and to process. Since tuples flow using the same reading order, if data doesn’t change, the evaluation of the branch predicates against every tuple that flows along the branch will not change. The result of predicate evaluation will be the same as the last time it was evaluated. To avoid subsequent evaluation of unchanged data tuples, we extended the SPIN approach with a bitset processing approach. A branch bitset (bitmap) is built according to the branch’ predicates, where each bit represents the boolean result of the predicate evaluation (true/false) applied to a corresponding tuple index. Future evaluations of the tuple can take advantage of the existence of this bitset, since the selection operator that evaluates the predicate can be replaced by a fast lookup operator to the corresponding position in the bitset to gathers the result. Bitsets are small and reside in memory in order to avoid introducing overhead at IO level. This is particularly relevant for predicates with high evaluations costs. Through the analysis of the data path (branches) of queries, and the required computational costs of each branch, it is possible to determine high accurate estimations of query execution times. Therefore, predictable execution times can be given for massive workload scalability. Tighter right-time guarantees can be provided by extending the parallel infrastructure, and redistributing data among processing nodes, but also by redistributing queries, query processing and data branches between nodes holding replicated fragments. This is achieved by using two distinct approaches, a parallel fine-tuned fragment level processing, named CARROUSEL, and an early-end query processing mechanism. CARROUSEL is a flexible fragment processor that uses idle nodes, or nodes currently running less time-stricter queries, to process some of the fragments required by time-stricter queries, on behalf of the fragment node’s owner. By reducing the data volume to be processed by a node, it can provide faster execution times. Alternatively, it may distribute some logical data branches among nodes with replicated fragments, and thus reducing query processing. This is only possible with nodes with replicated data fragments. The execution of a query ends when all tuples of the data fragments are processed and the circular logical loop is completed. But as the system is continuously spinning, reading and processing over and over the same data, it collects insightful information regarding the data that is stored in each data fragment. For some logical data branches, this can be relevant to reduce memory and computational usage by using a postponed start (delaying the query execution until the first relevant fragment is loaded) and early-end approaches (detaching the query pipeline when all the relevant fragments for a query have been processed). This information is useful when the architecture needs to perform a data rebalancing process, with the rebalanced data being clustered according to logical branch predicates and stored as new data fragments.
Thesis Type - Doctoral Thesis
URI - http://hdl.handle.net/10316/27097
Title - Automatic Heart Sound Analysis for Cardiovascular Disease Assessment
Author - Dinesh Kumar[https://estudogeral.sib.uc.pt/browse?type=author&value=Kumar%2C+Dinesh]
Advisor - Paulo Carvalho[https://estudogeral.sib.uc.pt/browse?type=author&value=Carvalho%2C+Paulo];Manuel Antunes[https://estudogeral.sib.uc.pt/browse?type=author&value=Antunes%2C+Manuel];
Keywords - heart sound;valvular disease;prosthetic valve;segmentation;heart murmur;classification;noise detection;time-frequency analysis;wavelet decomposition;nonlinear dynamics;som cardíaco;doença valvular;válvula prostética;segmentação;classificação;análise tempo-frequência;decomposição de onduletas;dinâmica não linear
Date - 30-Jun-2015
Abstract - Cardiovascular diseases (CVDs) are the most deadly diseases worldwide leaving behind diabetes and cancer. Being connected to ageing population above 65 years is prone to CVDs; hence a new trend of healthcare is emerging focusing on preventive health care in order to reduce the number of hospital visits and to enable home care. Auscultation has been open of the oldest and cheapest techniques to examine the heart. Furthermore, the recent advancement in digital technology stethoscopes is renewing the interest in auscultation as a diagnosis tool, namely for applications for the homecare context. A computer-based auscultation opens new possibilities in health management by enabling assessment of the mechanical status of the heart by using inexpensive and non-invasive methods. Computer based heart sound analysis techniques facilitate physicians to diagnose many cardiac disorders, such as valvular dysfunction and congestive heart failure, as well as to measure several cardiovascular parameters such as pulmonary arterial pressure, systolic time intervals, contractility, stroke volume, etc. In this research work, we address the problem of extracting a diagnosis using anal-ysis of the heart sounds. Heart sound analysis consists of three main tasks: i) identification of non-cardiac sounds which are unavoidably mixed with the heart sound during auscultation; ii) segmentation of the heart sound in order to localize the main sound component; and finally, iii) classification of the abnormal heart sounds from the normal heart sounds and in case of abnormal sounds classification is performed further to identify the type of the abnormal sound. In most previous works on heart sound analysis these three problems were tackled jointly. Two main classes of approaches are found in past years: first is to perform analysis using an auxiliary signal, such as ECG, sound signal for ambient noise, carotid pulse, etc which is acquired during auscultation; and the second is without the support of any auxiliary signal. The first group of methods can be ruled out for the sake of keeping analysis cost effective and convenient to the users, however, the second group of approaches is dependent on the use of regularity of heart sound components which may not be found in various cardiac disorders. Hence, our intention aims to develop algorithms for the each problem related to heart sound analysis with the following main requirements: i) not to use any auxiliary; ii) applicable to any age, weight, sex, body proportion subject; iii) robustness. Noise detection technique uses the template matching based approach. We pro-pose a new method applicable in real time to detect ambient and internal body noises manifested in heart sound during acquisition. The algorithm is designed on the basis of the periodic nature of heart sounds and physiologically inspired criteria. A small segment of uncontaminated heart sound exhibiting periodicity in time as well as in the time-frequency domain is first detected and applied as a reference signal in discriminating noise from the sound. For the segmentation problem, the heart is considered as a nonlinear dynamical system. In a first processing stage, Lyapunov exponents are computed from the phase space in order to identify the presence of murmur in the heart sound. Based on this information, the method enters one of two segmentation methods: for heart sound with murmur, an algorithm based on adaptive wavelet-nonlinear feature analysis is applied; for heart sounds without murmur, a less complex approach is followed based on the signal’s envelope analysis. Recognition of S1 and S2 sounds is achieved by in-specting high-frequency signatures. This marker is physiologically motivated by the accentuated pressure differences found across heart valves, both in native and pros-thetic valves, which leads to distinct high-frequency signatures of the valve closing sounds. Since heart murmurs originate from numerous anomalies in the heart, these show different characteristics, temporal, frequency and complexity. These features are ex-tracted in order to classify heart murmur using a supervised classifier. A new set of 17 features extracted in the time, frequency and in the state space domain is suggested. The features applied for murmur classification are selected using the floating sequen-tial forward method (SFFS). Using this approach, the original set of 17 features is re-duced to 10 features. The classification results achieved using the proposed method are compared on a common database with the classification results obtained using the feature sets proposed in two well-known state of the art methods for murmur classifica-tion. These algorithms have been tested on the database prepared with the help of the University Hospital in Coimbra. Three classes of the heart sound databases were pre-pared, i) normal heart sounds from the native valves; ii) abnormal heart sounds from the native valves; iii) heart sounds from artificial valve implants. Experimental results suggest that the algorithms can be applicable for cardiac application. As doenças cardiovasculares são a maior causa de morte em todo o mundo, ultrapassando de forma significativa a mortalidade devida aos diabetes e ao cancro. Dado o envelhecimento acentuado da população mundial e atendendo a que a incidência das doenças crónicas, em particular das doenças cardiovasculares, está fortemente correlacionada com a idade, observa-se uma nova tendência de cuidados de saúde focada nos cuidados de saúde preventivos, com vista a reduzir o número de episódios agudos numa tentativa de reduzir custos e de melhor a qualidade de vida dos pacientes. A implementação destas estratégias requer sistemas fiáveis de telemonitorização, porém simples e baratos por forma a permitir o seu uso prolongado por populações pouco motivadas e muitas vezes com iliteracia tecnológica. A auscultação é a técnica mais antiga e menos onerosa para examinar o coração. Os recentes avanços em estetoscópios com tecnologia digital está a renovar o interesse no uso da auscultação como ferramenta de diagnóstico, nomeadamente para aplica- ções no contexto da tele-monitorização e da gestão da doença. Uma auscultação assistida por computador abre novas possibilidades na gestão da saúde ao permitir a avaliação do estado mecânico do coração, usando-se métodos não invasivos e pouco onerosos. As técnicas de análise de som cardíaco assistidas por computador facilitam o diagnóstico médico de muitos problemas cardíacos, tais como a disfunção valvular e a insuficiência cardíaca congestiva, permitindo também medir vários parâmetros cardiovasculares, como por exemplo a pressão arterial pulmonar, intervalos de tempo sistólicos, contractilidade, volume sistólico, entre outros. Neste trabalho de investigação abordamos o problema de produzir um diagnóstico usando a análise de sons cardíacos. De uma forma geral, a análise de sons cardíacos consiste em três tarefas principais: i) na identificação de contaminações por sons não cardíacos, que se misturam inevitavelmente com o som do coração durante a auscultação; ii) na segmentação do som cardíaco por forma a localizar as sua principais componentes; e finalmente iii) na classificação dos sons cardíacos anormais e distin- ção dos sons cardíacos normais. No caso da existência de sons anormais, procede-se a uma classificação, por forma a identificar o tipo de som irregular. Na literatura sobre análise de som cardíaco, estes três problemas têm sido abordados conjuntamente. De uma forma geral, podemos afirmar que emergiram duas classes de abordagens principais: a primeira classe consiste na condução da análise usando um sinal auxiliar, tal como o ECG, um sinal sonoro para o ruído ambiente, o pulso da carótida, etc., sinal esse que é adquirido durante a auscultação; na segunda classe de métodos a tarefa é realizada sem o suporte a qualquer sinal auxiliar. O primeiro grupo de métodos não é muito prático para aplicações clínicas, dado o seu custo superior e o facto de exigir um número de sensores mais elevado o que, necessariamente, obriga a uma complexidade logística e operacional superior. No entanto, o segundo grupo de abordagens depende do uso da regularidade de componentes do som cardíaco, que poderá não ser detectada em vários problemas cardíacos. Deste modo, o objectivo deste trabalho é o de contribuir para a solução de cada um dos problemas principais relacionados com a análise do som cardíaco, dando resposta a alguns dos requisitos principais para uso clínico, ou seja: i) simplicidade pela ausência de uso de sinais auxiliares; ii) capacidade de generalização, isto é, aplicabilidade em populações heterogéneas; iii) robustez. Na metodologia proposta para a detecção de ruído usa-se uma abordagem baseada em correspondência de modelos (template matching). Propomos um novo método aplicável em tempo real para detectar contaminações por fontes de ruído ambiente e fontes de ruído fisiológicos que se manifestam no som cardíaco durante a sua captura. O algoritmo está concebido com base na natureza periódica dos sons cardíacos e em critérios fisiológicos. Um pequeno segmento de um som cardíaco não contaminado é primeiramente detectado e aplicado como um sinal de referência na distinção de ruído no som cardíaco. Para o problema da segmentação, o coração é considerado como um sistema dinâmico não linear. Numa primeira fase de processamento, os expoentes de Lyapunov são computados a partir do espaço de fase, por forma a avaliar a presença de murmúrios no som cardíaco. Com base nesta informação, o método usa um de dois algoritmos de segmentação: para o som cardíaco com murmúrio, aplica-se um algoritmo baseado numa análise de características de onduletas não lineares adaptativas; para sons cardíacos sem murmúrio, segue-se uma abordagem menos complexa, baseada na análise de envelope do sinal. O reconhecimento dos sons S1 e S2 é conseguida através do exame de uma assinatura obtida com base na informação de alta frequência. Este marcador é motivado fisiologicamente pelas acentuadas diferenças de pressão encontradas nas válvulas cardíacas, sejam estas naturais ou prostéticas, o que origina assinaturas de alta frequência distintas nos sons de fecho destes elementos. Uma vez que os murmúrios cardíacos derivam de várias anomalias no coração, estes últimos mostram características diferentes a nível temporal, de frequência e de complexidade. Estas características são extraídas por forma a classificar-se o murmúrio cardíaco usando um classificador supervisionado. Sugerimos um novo conjunto de 17 características extraídas no tempo, frequência e no domínio do espaço de fase. As características usadas para a classificação de murmúrios são seleccionadas usando o método SFFS (floating sequential forward method). Ao usarmos esta abordagem, o conjunto original de 17 características é reduzido para 10. Os resultados da classificação conseguidos usando o método proposto são comparados com dois métodos do estado da arte usando uma base de dados comum
Thesis Type - Doctoral Thesis
URI - http://hdl.handle.net/10316/27015
Title - Cardiovascular Performance Assessment for p-Health Applications
Author - Ricardo Jorge dos Santos Couceiro[https://estudogeral.sib.uc.pt/browse?type=author&value=Couceiro%2C+Ricardo+Jorge+dos+Santos]
Advisor - Paulo Carvalho[https://estudogeral.sib.uc.pt/browse?type=author&value=Carvalho%2C+Paulo];Rui Pedro Paiva[https://estudogeral.sib.uc.pt/browse?type=author&value=Paiva%2C+Rui+Pedro];
Keywords - p-Health;photoplethysmogram;electrocardiogram;motion artifacts;cardiac function;systolic time intervals;vascular function;arterial blood pressure regulation;neurally mediated syncope
Date - 10-Apr-2015
Abstract - As doenças cardiovasculares (CVDs) são atualmente a principal causa de morte no mundo e são responsáveis por mais de 7 milhões de mortes todos os anos. A mortalidade decorrente das CVDs tem vindo aumentar, principalmente devido ao crescimento da população nos países de baixo e médio rendimento, que alojam cerca de 85% da população mundial. Nos países de elevado rendimento, o acesso a melhores tecnologias de diagnostico e melhores terapêuticas, bem como estilos de vida mais saudáveis, inverteram esta tendência e a mortalidade resultante das CVDs está a decrescer. Este facto, aliado ao aumento da esperança média de vida das populações, leva a que as pessoas sejam afectadas ou morram devido a CVDs em idades mais avançadas, contribuindo para o aumento dos gastos com a saúde em todo o mundo. Uma condição que contribui largamente para este problema é a síncope, que têm um impacto económico equivalente a doenças como a asma, HIV e doença pulmonar obstrutiva crónica. Mais conhecida como “desmaio”, a síncope está associada a uma frequência elevada de quedas e de hospitalizações, e é responsável por uma menor a qualidade de vida, especialmente em populações mais idosas. Para enfrentar os encargos socioeconómicos derivados das CVDs, o paradigma da saúde tem vindo a mudar de reativo e centralizado nos hospitais para preventivo e centrado em cada individuo, com um foco especial no diagnostico precoce e em melhores estratégias de prevenção e gestão das CVDs. Assim, o desenvolvimento de novas metodologias para monitorização da função cardiovascular, capazes de serem aplicadas em sistemas de baixo custo, não invasivos e portáteis, são essenciais para a prevenção e controlo desta crescente epidemia que são as CVDs. Apesar dos recentes avanços tecnológicos, as técnicas padrão atuais para a avaliação da função cardiovascular, como a ressonância magnética cardíaca e ecocardiografia, ainda apresentam várias limitações no que diz respeito à sua aplicação em ambientes de saúde personalizada. Assim, a utilização de modalidades amplamente disponíveis e de baixo custo, como o eletrocardiograma e o fotopletismograma, para a avaliação não-invasiva, contínua e de longo prazo da função cardiovascular pode ser a chave para melhores estratégias de prevenção e gestão de doenças cardiovasculares. Mais concretamente, a extração de parâmetros cardiovasculares a partir destas modalidades pode ser crucial na predição de síncopes e prevenção de quedas. A principal contribuição da presente tese consiste no desenvolvimento de novos algoritmos para a avaliação continua, não invasiva e robusta da função cardiovascular, com base na análise do eletrocardiograma e do fotopletismograma. Visto que o fotopletismograma é facilmente afectado por ruído e artefactos de movimento, o que representa um obstáculo para a extração de parâmetros cardiovasculares, é fundamental detectar quais as secções do fotopletismograma passiveis de serem posteriormente analisadas. Assim, propomos um novo método para detecção de artefactos de movimento baseado na extração e análise de características do domínio temporal e de período. Consequentemente, propomos um novo algoritmo para a estimação do tempo de ejecção do ventrículo esquerdo, o qual está associado com a função cardíaca, bem como outros parâmetros relacionados com alterações de pressão sanguínea e de tónus vascular. Finalmente, propomos um novo algoritmo para a predição de síncopes (mais especificamente, síncope neuromediada) baseada na avaliação dos parâmetros previamente extraídos. Os métodos propostos foram validados em três bases de dados, coligidas no Departamento de Engenharia Informática da Universidade de Coimbra, no Centro Hospitalar da Universidade de Coimbra e no departamento de Eletrofisiologia do Centro Universitário do Coração, Hospital Universitário de Eppendorf, Hamburgo, Alemanha. Cardiovascular diseases (CVDs) are currently the leading cause of death in the world and are responsible for over 17 million deaths per year. The mortality of CVDs is increasing, mainly driven by the increase of the population in low and middle income countries, which house about 85% of the world’s population. In high-income countries, the access to better diagnostic and therapeutic technologies, as well as healthier life stiles, reverses this tendency and CVD mortality is decreasing. In combination with the increase in the populations’ life expectancy, people are affected or die as a result of CVD at older ages, contributing to the rise in the health care expenditures all over the world. A condition largely contributing to this matter is syncope, which has an economic impact equivalent to conditions such as asthma, HIV, and chronic obstructive pulmonary disease. More commonly known as fainting, syncope is associated with high rate of falls and hospitalization and is responsible for reducing lifestyle quality, especially in the elderly. To face this socioeconomic burden caused by CVDs, the health care paradigm is shifting from a reactive hospital-centered to a preventive individual-centered care, with special emphasis in earlier diagnosis and better prevention and management strategies. Therefore, the development of new methodologies for monitoring the cardiovascular function, capable of being applied in low-cost, non-invasive and portable systems, are essential to prevent and control the evolving epidemic of CVDs. Despite the recent technological advances, the current standard techniques for the assessment of cardiovascular function, such as the cardiac magnetic resonance and echocardiography, still exhibit several limitations in what concerns to their application in personal health environments. Therefore, the use of widely available and cost-effective modalities such as the electrocardiogram and photoplethysmogram, for the non-invasive, continuous and long-term assessment of the cardiovascular function may be the key to provide a better prevention and management strategies of CVDs. More specifically, the extraction of cardiovascular parameters from these modalities may be crucial in the prediction of syncope events and prevention of falls. The key contribution of the present thesis is the development of new algorithms for the continuous, non-invasive and robust assessment of cardiovascular function, based on the analysis of the electrocardiogram and photoplethysmogram. Since the photoplethysmogram is easily influenced by noise and motion artifacts, which can be a serious obstacle in the extraction of cardiovascular parameters, it is essential to detect which sections of the photoplethysmogram are liable for further analysis. Therefore, we propose a new method for the detection of motion artifacts, based on the extraction and analysis of time and period domain features. Consequently, we propose a new algorithm for the assessment of the left ventricular ejection time, which is associated with the cardiac function, among other parameters related with blood pressure and vascular tones changes. Finally, we propose a new algorithm for the prediction of syncope events (more specifically, neurally mediated syncope), based on the evaluation of changes in the previously extracted cardiovascular parameters. The proposed methods were validated in three databases collected in the Department of Informatics Engineering of the University of Coimbra, in the Hospital Center of University of Coimbra and in the Department of Electrophysiology of the University Heart Center, University Hospital Eppendorf, Hamburg, Germany.
Thesis Type - Doctoral Thesis
URI - http://hdl.handle.net/10316/26987
Title - End-to-end security solutions for Internet-integrated Wireless Sensor Networks
Author - António Jorge da Costa Granjal[https://estudogeral.sib.uc.pt/browse?type=author&value=Granjal%2C+Ant%C3%B3nio+Jorge+da+Costa]
Advisor - Edmundo Monteiro[https://estudogeral.sib.uc.pt/browse?type=author&value=Monteiro%2C+Edmundo];Jorge Sá Silva[https://estudogeral.sib.uc.pt/browse?type=author&value=Silva%2C+Jorge+S%C3%A1];
Keywords - Security in Wireless Sensor Networks;End-to-end security in 6LoWPAN environments
Date - 21-Nov-2014
Abstract - A investigação em soluções tecnológicas para as Redes de Sensores Sem Fios (RSSF) despertou grande interesse e inúmeros esforços ao nível da investigação em anos recentes. O objetivo inicial de tais redes foi o de providenciar uma base tecnológica que permitisse dispor de aplicações sensoriais distribuídas, desenhadas com propósitos bem específicos nas mais diversas áreas de investigação e aplicação. Uma característica distintiva das RSSF é a utilização de dispositivos com capacidade para comunicar por radiofrequência e de “sentir” e “atuar” no meio físico que os rodeia. Tal capacidade permite, na prática, o desenvolvimento e a utilização de soluções verdadeiramente inovadoras implementadas com recurso a aplicações distribuídas capazes de interagir com o mundo físico. As aplicações originais das RSSF visavam essencialmente a construção de soluções eficientes para problemas bem delimitados e, como consequência, tais redes não eram projetadas com o objetivo de suportar diferentes tipos de aplicações ou mecanismos de comunicação adaptáveis a diferentes propósitos de utilização. Podemos igualmente verificar que os mecanismos de comunicação e segurança utilizados em tais aplicações eram desenhados de acordo com o seu propósito específico de utilização. Por conseguinte, a heterogeneidade ao nível das aplicações suportadas e dos seus requisitos não foi considerado um aspecto prioritário no desenho das soluções clássicas de comunicação e segurança em ambientes de RSSF. Tais soluções consideram portanto as RSSF como sendo vocacionadas essencialmente para o suporte de aplicações especializadas e isoladas, sem a capacidade de suportar aplicações e equipamentos heterogéneos, bem como comunicações com dispositivos externos ao seu ambiente de comunicações. Tais características são fundamentais na evolução e no sucesso da infraestrutura de comunicações e segurança da Internet, e motivam atualmente uma mudança de paradigma ao nível da utilização das RSSF, um aspeto que motiva as abordagens de investigação descritas na presente tese. A viabilidade da maioria das aplicações dos ambientes RSSF, quer isolados ou integrados com a Internet, dependerá fortemente da utilização de mecanismos adequados de segurança. Embora a segurança seja fundamental no contexto das várias estratégias de integração analisadas na presente tese, pode ser considerada particularmente relevante no contexto da exposição das RSSF à infraestrutura de comunicações global da Internet. De facto, para além das ameaças de segurança inerentes à utilização de comunicações sem fios e às restrições dos próprios dispositivos sensoriais, a exposição das RSSF à infraestrutura de comunicações Internet, ainda que limitada e controlada, motivará necessariamente riscos e ameaças acrescidas que importa prevenir e combater através da adopção de mecanismos de segurança apropriados. Os trabalhos de investigação apresentados na presente tese abordam a problemática da segurança no contexto da integração das RSSF com a Internet, em particular no que diz respeito à proteção das comunicações fim-a-fim no contexto da integração das RSSF com a infraestrutura de comunicações e segurança global. Esta estratégia de integração está presentemente a ganhar protagonismo através da adopção de mecanismos Internet de comunicações optimizados para ambientes RSSF. Um objetivo central das propostas de investigação discutidas na presente tese é precisamente o de contribuir para os ambientes de RSSF que utilizam tais mecanismos de comunicação, em particular no que concerne à proteção de comunicações fim-a-fim entre dispositivos SSF e sistemas externos às RSSF. Os mecanismos de segurança propostos ao longo da presente tese abordam várias técnicas e focam-se em diferentes níveis protocolares, para a obtenção de segurança fim-a-fim com RSSF integradas com a Internet. Estes mecanismos procuram responder, em particular, à questão da viabilidade de obtenção, de forma eficiente, de segurança efetiva no contexto da utilização de comunicações fim-a-fim Internet entre SSF e outros sistemas externos à RSSF ou na Internet. Tais mecanismos de segurança e comunicações poderão contribuir de forma decisiva para a viabilidade de aplicações sensoriais distribuídas que dependam ou beneficiem de comunicações Internet diretas entre sistemas Internet e plataformas sensoriais com restrições ao nível de recursos tais como os SSF. De notar que, apesar do foco particular da presente tese nas soluções de segurança para comunicações fim-a-fim, a integração das RSSF com a Internet motivará igualmente a utilização de outros tipos de mecanismos desenhados para fazer face a requisitos de segurança muitas vezes transversais aos vários protocolos de comunicação. Tais mecanismos podem garantir funcionalidades de segurança tais como o controlo de acessos, a gestão de chaves ou a detecção de intrusões, entre outros, ou suportar mecanismos para garantia de aspectos de segurança tal como a privacidade e a confiança. Os mecanismos desenhados e avaliados ao longo da presente tese consideram diferentes aproximações ao problema da obtenção de segurança fim-a-fim em RSSF integradas com a Internet. Estes mecanismos são avaliados de acordo com a sua capacidade de cumprirem requisitos de segurança predefinidos, ao mesmo tempo fazendo uma utilização eficiente dos recursos críticos e limitados ao seu dispor nas RSSF. O trabalho de investigação descrito na presente tese refere-se ao desenho de mecanismos para obtenção de segurança fim-a-fim ao nível das camadas protocolares de rede, transporte e aplicação. À semelhança da arquitetura atual de comunicações da Internet, a utilização de mecanismos complementares de comunicações e segurança também promove o suporte efetivo de aplicações e cenários de utilização com diferentes características e requisitos. The area of Wireless Sensor Networks (WSN) has motivated great interest and numerous research efforts in the recent years. The initial purpose of these networks was to provide a technological basis on top of which new distributed sensorial applications can be built. One main distinctive characteristic of WSN is the employment of sensing devices that have the capability of communicating wirelessly, and also of “feeling” and “actuating“ with the physical world. Such capabilities enable the development of truly innovative solutions, based on applications that are designed to benefit from or require interactions with the physical world. Most traditional WSN approaches target particular research goals and applications with very focused purposes, rather than the support of heterogeneous applications and devices as in traditional Internet communication environments. Due to the characteristics and constraints of WSN sensing devices, the communication and security technologies designed for such applications were optimized according to the particular requirements of the application at hand, rather than to support heterogeneous applications and devices, as is the traditional Internet communications environment. As in traditional WSN applications, security will be a fundamental enabling factor of future sensorial applications employing sensing devices integrated with the Internet communications infrastructure. This applies to all the existing integration approaches, and will constitute a particularly relevant and challenging aspect for the integration of WSN employing Internet communication technologies. In such WSN environments, security threats will be present not only because of aspects which are inherent of WSN environments, for example the employment of wireless communications and the constraints and physical exposure of sensing devices, but also because of the threats which may be present from the day we start exposing WSN communications to the Internet. If on the one side security mechanisms, such as traffic filtering or intrusion detection, may help in preventing such threats, on the other applications may require or benefit from the employment of true end-to-end communications involving constrained sensing devices. Security will thus be of paramount importance for the enabling of such applications. In the present thesis we describe and evaluate research proposals designed to target the problem of security in the context of WSN integrated with the Internet using Internet communication technologies designed and optimized for such environments. These technologies enable end-to-end Internet communications between WSN devices, and also between WSN devices and external or Internet hosts, and provide the context for the research proposals described in the present thesis. We target different approaches in supporting end-to-end security in the context of Internet-integrated WSN, with the goal of investigating the viability of supporting end-to-end security with communication technologies developed for such environments, and providing complementary solutions to support heterogeneous applications and deployment environments. We must also note that, despite our particular focus on end to end communications and security, the full integration of WSN with the Internet will in fact require efforts towards the design of appropriate mechanisms targeting other important security aspects. Such mechanisms may possibly be designed in a cross-layer fashion and support fundamental security-related operations such as key management and intrusion detection, or the enforcement of security requirements such as privacy and trust, among others. The security proposals discussed in the thesis seek to investigate the viability of enabling security for end-to-end communications with sensing devices using Internet WSN communication technologies. For this purpose, we propose solutions to protect communications using technologies currently being designed without proper security mechanisms, and propose alternatives to existing security approaches that we find to be inappropriate or insufficient. The research solutions proposed and evaluated throughout the thesis also aims to support heterogeneous devices and applications, as security is addressed at different protocol layers and by implementing different approaches to the support of security-related procedures. The effectiveness of new security mechanisms may be measured according to their ability to not compromise the lifetime of sensing applications, in the light of the previously discussed characteristics and constraints of WSN applications and devices, which we may measure according to specific metrics and evaluation criteria. On the other end, applications with different functional and security requirements must also be appropriately supported by the proposed mechanisms, in line with our goal of securing communications supporting heterogeneous applications and deployment environments. The security solutions proposed and evaluated throughout the thesis are materialized in security mechanisms implementing different approaches to the problem of end-to-end security with Internet-integrated WSN. We evaluate the proposed solutions against its ability to cope with predefined security requirements, while at the same time being able to employ the limited resources available on constrained sensing platforms in an efficient and controlled manner. As with the current Internet architecture, the complementarity of the approaches considered for the design of such mechanisms may promote the support of applications and deployment scenarios with different characteristics and requirements in terms of security.
Thesis Type - Doctoral Thesis
URI - http://hdl.handle.net/10316/25654
Title - A Computational Model Inspired by Gene Regulatory Networks
Author - Rui Miguel Lourenço Lopes[https://estudogeral.sib.uc.pt/browse?type=author&value=Lopes%2C+Rui+Miguel+Louren%C3%A7o]
Advisor - Ernesto Costa[https://estudogeral.sib.uc.pt/browse?type=author&value=Costa%2C+Ernesto];
Keywords - Computational Evolution;Genetic Programming;Representations;Gene Regulatory Networks
Date - 27-Jan-2015
Abstract - Evolutionary Algorithms (EA) are parallel stochastic search procedures that are loosely inspired by the concepts of natural selection and genetic heredity. They have been successfully applied to many domains, and today Evolutionary Computation (EC) attracts a growing number of researchers from the most varied fields. The end of the 20th century brought uncountable discoveries in the biological realm, enabled by the underlying technological breakthroughs. Complete genomes have been sequenced, including the human one, and thanks to the increasing interdisciplinarity of researchers it is known today that there is much more to evolution than just natural selection, namely the influence of the environment, gene regulation, and development. At the core of these processes there is a fundamental piece of complex biological machinery, the Genetic Regulatory Network (GRN). This network results from the interaction amongst the genes and proteins, as well as the environment, governing gene expression and consequently the development of the organism. It is a true fact that the biological knowledge has advanced faster than our ability to incorporate it into the EAs, despite of whether or not it is benificial to do so. One of the main critics pointed-out is that the approach to the genotype-phenotype relationship is different from nature. A lot of effort has been put by some researchers into developing new representations, achieving not only improved benchmark results, but also extended flexibility and applicability of the algorithms. Moreover, others have recently started exploring computationally the new comprehension of the multitude of regulatory mechanisms that are fundamental in both the processes of inheritance and of development in natural systems, by trying to include those mechanisms in the EAs. However, few of these target machine learning problems, and most are usually developed with a specific problem domain in mind. The work presented here addresses this issue by incorporating a model of GRN in a Genetic Programming (GP) architecture. This thesis main contribution is a model that incorporates Artificial Regulatory Networks as the genotypic representation in a GP-like system, and an algorithm that maps these networks into executable program graphs. Moreover, variants of the model were also developed, extending the capabilities of the approach to classes of problems with recursive definitions, and with multiple outputs. The efficacy and efficiency of this alternative were tested experimentally using typical benchmark problems for Genetic Programming (GP) systems, from regression to control, and logic design. Despite some limitations that were identified, the analysis of the results shows that this new method is competitive in most problem domains, even outperforming the state-of-the-art results in some cases.
Thesis Type - Doctoral Thesis
URI - http://hdl.handle.net/10316/25599
Title - Enhancing Data Security in Data Warehousing
Author - Ricardo Santos[https://estudogeral.sib.uc.pt/browse?type=author&value=Santos%2C+Ricardo]
Advisor - Jorge Bernardino[https://estudogeral.sib.uc.pt/browse?type=author&value=Bernardino%2C+Jorge];Marco Vieira[https://estudogeral.sib.uc.pt/browse?type=author&value=Vieira%2C+Marco];
Keywords - Data Security;Data Warehousing;Data Masking;Encryption;Database Intrusion Detection;Database Security Frameworks
Date - 22-Dec-2014
Abstract - Data Warehouses (DWs) store sensitive data that encloses many business secrets. They have become the most common data source used by analytical tools for producing business intelligence and supporting decision making in most enterprises. This makes them an extremely appealing target for both inside and outside attackers. Given these facts, securing them against data damage and information leakage is critical. This thesis proposes a security framework for integrating data confidentiality solutions and intrusion detection in DWs. Deployed as a middle tier between end user interfaces and the database server, the framework describes how the different solutions should interact with the remaining tiers. To the best of our knowledge, this framework is the first to integrate confidentiality solutions such as data masking and encryption together with intrusion detection in a unique blueprint, providing a broad scope data security architecture. Packaged database encryption solutions are been well-accepted as the best form for protecting data confidentiality while keeping high database performance. However, this thesis demonstrates that they heavily increase storage space and introduce extremely large response time overhead, among other drawbacks. Although their usefulness in their security purpose itself is indisputable, the thesis discusses the issues concerning their feasibility and efficiency in data warehousing environments. This way, solutions specifically tailored for DWs (i.e., that account for the particular characteristics of the data and workloads are capable of delivering better tradeoffs between security and performance than those proposed by standard algorithms and previous research. This thesis proposes a reversible data masking function and a novel encryption algorithm that provide diverse levels of significant security strength while adding small response time and storage space overhead. Both techniques take numerical input and produce numerical output, using data type preservation to minimize storage space overhead, and simply use arithmetical operators mixed with eXclusive OR and modulus operators in their data transformations. The operations used in these data transformations are native to standard SQL, which enables both solutions to use transparent SQL rewriting to mask or encrypt data. Transparently rewriting SQL allows discarding data roundtrips between the database and the encryption/decryption mechanisms, thus avoiding I/O and network bandwidth bottlenecks. Using operations and operators native to standard SQL also enables their full portability to any type of DataBase Management System (DBMS) and/or DW. Experimental evaluation demonstrates the proposed techniques outperform standard and state-of-the-art research algorithms while providing substantial security strength. From an intrusion detection view, most Database Intrusion Detection Systems (DIDS) rely on command-syntax analysis to compute data access patterns and dependencies for building user profiles that represent what they consider as typical user activity. However, the considerable ad hoc nature of DW user workloads makes it extremely difficult to distinguish between normal and abnormal user behavior, generating huge amounts of alerts that mostly turn out to be false alarms. Most DIDS also lack assessing the damage intrusions might cause, while many allow various intrusions to pass undetected or only inspect user actions a posteriori to their execution, which jeopardizes intrusion damage containment. This thesis proposes a DIDS specifically tailored for DWs, integrating a real-time intrusion detector and response manager at the SQL command level that acts transparently as an extension of the database server. User profiles and intrusion detection processes rely on analyzing several distinct aspects of typical DW workloads: the user command, processed data and results from processing the command. An SQL-like rule set extends data access control and statistical models are built for each feature to obtain individual user profiles, using statistical tests for intrusion detection. A self-calibration formula computes the contribution of each feature in the overall intrusion detection process. A risk exposure method is used for alert management, which is proven more efficient in damage containment than using alert correlation techniques to deal with the generation of high amounts of alerts. Experiments demonstrate the overall efficiency of the proposed DIDS. As Data Warehouses (DWs) armazenam dados sensíveis que muitas vezes encerram os segredos do negócio. São actualmente a forma mais utilizada por parte de ferramentas analíticas para produzir inteligência de negócio e proporcionar apoio à tomada de decisão em muitas empresas. Isto torna as DWs um alvo extremamente apetecível por parte de atacantes internos e externos à própria empresa. Devido a estes factos, assegurar que o seu conteúdo é devidamente protegido contra danos que possam ser causados nos dados, ou o roubo e utilização ou divulgação desses dados, é de uma importância crítica. Nesta tese, é apresentada uma framework de segurança que possibilita a integração conjunta das soluções de confidencialidade de dados e detecção de intrusões em DWs. Esta integração conjunta de soluções é definida na framework como uma camada intermédia entre os interfaces dos utilizadores e o servidor de base de dados, descrevendo como as diferentes soluções interagem com os restantes pares. Consideramos esta framework como a primeira do género que combina tipos distintos de soluções de confidencialidade, como mascaragem e encriptação de dados com detecção de intrusões, numa única arquitectura integrada, promovendo uma solução de segurança de dados transversal e de grande abrangência. A utilização de pacotes de soluções de encriptação incluídos em servidores de bases de dados tem sido considerada como a melhor forma de proteger a confidencialidade de dados sensíveis e conseguir ao mesmo tempo manter um nível elevado de desempenho nas bases de dados. Contudo, esta tese demonstra que a utilização de encriptação resulta tipicamente num aumento extremamente considerável do espaço de armazenamento de dados e no tempo de processamento e resposta dos comandos SQL, entre outras desvantagens ou aspectos negativos relativos ao seu desempenho. Apesar da sua utilidade indiscutível no cumprimento dos pressupostos em termos de segurança propriamente ditos, nesta tese discutimos os problemas inerentes que dizem respeito à sua aplicabilidade, eficiência e viabilidade em ambientes de data warehousing. Argumentamos que soluções especificamente concebidas para DWs, que tenham em conta as características particulares dos seus dados e as actividades típicas dos seus utilizadores, são capazes de produzir um melhor equilíbrio entre segurança e desempenho do que as soluções previamente disponibilizadas por algoritmos standard e outros trabalhos de investigação para bases de dados na sua generalidade. Nesta tese, propomos uma função reversível de mascaragem de dados e um novo algoritmo de encriptação, que providenciam diversos níveis de segurança consideráveis, ao mesmo tempo que adicionam pequenos aumentos de espaço de armazenamento e tempo de processamento. Ambas as técnicas recebem dados numéricos de entrada e produzem dados numéricos de saída, usam preservação do tipo de dados para minimizar o aumento do espaço de armazenamento, e simplesmente utilizam combinações de operadores aritméticos conjuntamente com OU exclusivos (XOR) e restos de divisão (MOD) nas operações de transformação de dados. Como este tipo de operações se conseguem realizar recorrendo a comandos nativos de SQL, isto permite a ambas as soluções utilizar de forma transparente a reescrita de comandos SQL para mascarar e encriptar dados. Este manuseamento transparente de comandos SQL permite requerer a execução desses mesmos comandos ao Sistema de Gestão de Base de Dados (SGBD) sem que os dados tenham de ser transportados entre a base de dados e os mecanismos de mascaragem/desmascaragem e encriptação/ decriptação, evitando assim o congestionamento em termos de I/O e rede. A utilização de operações e operadores nativos ao SQL também permite a sua portabilidade para qualquer tipo de SGBD e/ou DW. As avaliações experimentais demonstram que as técnicas propostas obtêm um desempenho significativamente superior ao obtido por algoritmos standard e outros propostos pelo estado da arte da investigação nestes domínios, enquanto providenciam um nível de segurança considerável. Numa perspectiva de detecção de intrusões, a maioria dos Sistemas de Detecção de Intrusões em Bases de Dados (SDIBD) utilizam formas de análise de sintaxe de comandos para determinar padrões de acesso e dependências que determinam os perfis que consideram representativos da actividade típica dos utilizadores. Contudo, a carga considerável de natureza ad hoc existente em muitas acções por parte dos utilizadores de DWs gera frequentemente um número avassalador de alertas que, na sua maioria, se revelam falsos alarmes. Muitos SDIBD também não fazem qualquer tipo de avaliação aos potenciais danos que as intrusões podem causar, enquanto muitos outros permitem que várias intrusões passem indetectadas ou apenas inspeccionam as acções dos utilizadores após essas acções terem completado a sua execução, o que coloca em causa a possível contenção e/ou reparação de danos causados. Nesta tese, propomos um SDIBD especificamente concebido para DWs, integrando um detector de intrusões em tempo real, com capacidade de parar ou impedir a execução da acção do utilizador, e que funciona de forma transparente como uma extensão do SGBD. Os perfis dos utilizadores e os processos de detecção de intrusões recorrem à análise de diversos aspectos distintos característicos da actividade típica de utilizadores de DWs: o comando SQL emitido, os dados processados, e os dados resultantes desse processamento. Um conjunto de regras tipo SQL estende o alcance das políticas de controlo de acesso a dados, e modelos estatísticos são construídos baseados em cada variável relevante à determinação dos perfis dos utilizadores, sendo utilizados testes estatísticos para analisar as acções dos utilizadores e detectar possíveis intrusões. Também é descrito um método de calibragem automatizado da contribuição de cada uma dessas variáveis no processo global de detecção de intrusões, com base na eficiência que vão apresentando ao longo do tempo nesse mesmo processo. Um método de exposição de risco é definido para fazer a gestão de alertas, que é mais eficiente do que as técnicas de correlação habitualmente utilizadas para este fim, de modo a lidar com a geração de quantidades elevadas de alertas. As avaliações experimentais incluídas nesta tese demonstram a eficiência do SDIBD proposto.
Thesis Type - Doctoral Thesis
URI - http://hdl.handle.net/10316/25230
Title - An IEEE1451.0-compliant FPGA-based reconfigurable weblab
Author - Ricardo Costa[https://estudogeral.sib.uc.pt/browse?type=author&value=Costa%2C+Ricardo]
Advisor - Gustavo Alves[https://estudogeral.sib.uc.pt/browse?type=author&value=Alves%2C+Gustavo];Mário Rela[https://estudogeral.sib.uc.pt/browse?type=author&value=Rela%2C+M%C3%A1rio];
Keywords - Remote experimentation;Weblabs;Remote laboratories;IEEE1451.0 Std.;FPGAs;Hardware reconfiguration
Date - 22-May-2014
Abstract - Technology evolution is contributing for a sustainable change in engineering education. New resources and tools are continuously improving the teaching and learning processes, providing more pathways to both students and teachers for accessing better educational contents. In engineering courses, the experimental work, typically supported by traditional laboratories, is also encompassing technology evolution as denoted by the appearance of the so-called weblabs or remote laboratories. This type of laboratories allows both students and teachers to remotely access physical experiments enabling the control of laboratory equipment through a simple device connected to the Internet (e.g. a PC). Besides the provided flexibility (e.g. access to a real laboratory on a 24x7 basis) other advantages may be enumerated, such as the increase on students‟ motivation and the cost reductions for all the involved actors in the teaching and learning process (e.g. students, teachers, institutions, etc.). However, current weblabs‟ architectures and their underlying infrastructures follow specific and distinct technical implementations, i.e. there is no standard solution. Moreover, they are not able to be reconfigured with different instruments and modules, known as weblab modules. Whenever required in a traditional laboratory, these modules can be attached to the target experiments, provided that they are available in the laboratory facilities. Some weblabs‟ implementations allow setting up connections between the target experiments and the weblab modules provided in the infrastructure, but these modules cannot be changed or replicated, i.e. the flexibility for changing the layout and the modules used in a particular weblab infrastructure is very reduced. Therefore, the lack of a standard access and design of weblabs, and the reduced flexibility for changing the required modules for conducting the target experiments, are two issues that are preventing their wide-spread adoption in engineering education. This thesis describes a research work conducted to design standard-based reconfigurable weblabs. It analyses the possibility of using the IEEE1451.0 Std. to design the weblabs and the modules adopted by the underlying infrastructures to control/monitor the target experiments. Additionally, to provide reconfiguration capability to the weblab infrastructure, it considers the use of Field Programmable Gate Arrays (FPGAs) for accommodating the weblab modules, thus allowing: i) the use of standard Hardware Description Languages (HDLs) to describe the weblab modules, making them easily sharable and replicable and; ii) the weblab infrastructures to inherit the reconfigurable nature of FPGAs, making them flexible in order to accommodate different embedded modules with the inherent reduction of costs that may arise from replacing traditional with embedded instrumentation. Besides contextualizing the role of weblabs in engineering education, presenting some examples and commenting the use of traditional instrumentation standards for their design, the thesis describes the IEEE1451.0 Std., suggesting extensions for its adoption in the design of weblabs. Supported on those suggestions and on FPGA technologies, it specifies the development of an IEEE1451.0-compliant reconfigurable weblab prototype and presents and analyses researchers‟ opinions about its use and the benefits for engineering education.
Thesis Type - Doctoral Thesis
URI - http://hdl.handle.net/10316/24888
Title - Complexity and emergence in societies of agents
Author - Tiago Rodrigues Baptista[https://estudogeral.sib.uc.pt/browse?type=author&value=Baptista%2C+Tiago+Rodrigues]
Advisor - Ernesto Jorge Fernandes Costa[https://estudogeral.sib.uc.pt/browse?type=author&value=Costa%2C+Ernesto+Jorge+Fernandes];
Keywords - Sistemas complexos;Vida artificial;Sistemas multi-agente;Computação evolucionária
Date - 2012
Abstract - Throughout the last decades, Darwin’s theory of natural selection has fuelled a vast amount of research in the field of computer science, and more specifically in artificial intelligence. The majority of this work has focussed on artificial selection, rather than on natural selection. In parallel, a growing interest in complexity science brought new modelling paradigms into the scene, with a focus on bottom-up approaches. By combining ideas from complex systems research and artificial life, we present a multi-agent simulation model for open-ended evolution, and a software framework (BitBang) that implements it. We also present a rule list based algorithm implemented for the brain component of the agents. Genetic variation operators were created to drive the evolution of the rule list brains. Several simulation environments were created using the BitBang framework. Experimental results are presented and analysed, validating our model. The results presented show that the model is capable of evolving agents’ controllers in an open-ended evolution simulation. We see that populations evolve sustainable reproduction behaviours, without hard-coding the reproduction conditions into the simulations. By providing evolutionary pressure through the modelling of the environment, we see that on increasingly complex environments, agents evolve increasingly complex behaviours. The rule list brain is shown to provide an important analysis advantage by having readability into the agents’ evolved behaviours. This feature proved to be especially important when unexpected behaviours emerged.
Thesis Type - Doctoral Thesis
URI - http://hdl.handle.net/10316/24794
Title - A supporting infrastructure for Wireless Sensor Networks in Critical Industrial Environments
Author - Thanh-Dien Tran[https://estudogeral.sib.uc.pt/browse?type=author&value=Tran%2C+Thanh-Dien]
Advisor - Jorge Miguel Sá Silva[https://estudogeral.sib.uc.pt/browse?type=author&value=Silva%2C+Jorge+Miguel+S%C3%A1];
Keywords - Redes de Sensores sem fios;Localização;Integração;Internet das coisas;Comunicação fiável
Date - 22-Oct-2014
Abstract - As Redes de Sensores Sem Fios (RSSFs) têm uma aplicabilidade muito elevada nas mais diversas áreas, como na indústria, nos sistemas militares, na saúde e nas casas inteligentes. No entanto, continuam a existir várias limitações que impedem que esta tecnologia tenha uma utilização extensiva. A fiabilidade é uma destas principais limitações que tem atrasado a adopção das RSSFs em ambientes industriais, principalmente quando sujeitos a elevadas interferências e ruídos. Por outro lado, a interoperabilidade é também um dos principais requisitos a cumprir nomeadamente com o avanço para o paradigma da Internet of Things. A determinação da localização dos nós, principalmente dos nós móveis, é, também ele, um requisito crítico em muitas aplicações. Esta tese de doutoramento propõe novas soluções para a integração e para a localização de RSSFs que operem em ambientes industriais e críticos. Como os nós sensores são, na maioria das vezes, instalados e deixados sem intervenção humana durante longos períodos de tempo, isto é, meses ou mesmo anos, é muito importante oferecer processos de comunicação fiável. No entanto, muitos problemas ocorrem durante a transmissão dos pacotes, nomeadamente devido a ruídos, interferências e perda de potência do sinal. A razão das interferências deve-se à existência de mais do que uma rede ou ao espalhamento espectral que ocorre em determinadas frequências. Este tipo de problemas é mais severo em ambientes dinâmicos nos quais novas fontes de ruído pode ser introduzidas em qualquer instante de tempo, nomeadamente com a chegadas de novos dispositivos ao meio. Consequentemente, é necessário que as RSSFs tenham a capacidade de lidar com as limitações e as falhas nos processos de comunicação. O protocolo Dynamic MAC (DunMAC) proposto nesta dissertação utiliza técnicas de rádio cognitivo (CR) para que a RSSF se adapte, de forma dinâmica, a ambientes instáveis e ruidosos através da selecção automática do melhor canal durante o período de operação. As RSSFs não podem operar em isolação completa do meio, e necessitam de ser monitoradas e controladas por aplicações externas. Apesar de ser possível adicionar a pilha protocolar IP aos nós sensores, este procedimento não é adequado para muitas aplicações. Para estes casos, os modelos baseados em gateway ou proxies continuam a apresentar-se preferíveis para o processo de integração. Um dos desafios existentes para estes processos de integração é a sua adaptabilidade, isto é, a capacidade da gateway ou do proxy poder ser reutilizado sem alterações por outras aplicações. A razão desta limitação deve-se aos consumidores finais dos dados serem aplicações e não seres humanos. Logo, é difícil ou mesmo impossível criar normas para as estruturas de dados dada a infinidade de diferentes formatos. É então desejável encontrar uma solução que permita uma integração transparente de diferentes RSSFs e aplicações. A linguagem Sensor Traffic Description Language (STDL) proposta nesta dissertação propõe uma solução para esta integração através de gateways e proxies flexíveis e adaptados à diversidade de aplicações, e sem recorrer à reprogramação. O conhecimento da posição dos nós sensores é, também ele, crítico em muitas aplicações industriais como no controlo da deslocação dos objectos ou trabalhadores. Para além do mais, a maioria dos valores recolhidos dos sensores só são úteis quando acompanhados pelo conhecimento do local onde esses valores foram recolhidos. O Global Positioning Systems (GPS) é a mais conhecida solução para a determinação da localização. No entanto, o recurso ao GPS em cada nó sensor continua a ser energeticamente ineficiente e impraticável devido aos custos associados. Para além disso, os sistemas GPS não são apropriados para ambientes in-door. Este trabalho de doutoramento propõe-se actuar nestas áreas. Em particular, é proposto, implementado e avaliado o protocolo DynMAC para oferecer fiabilidade às RSSFs. Para a segunda temática, a linguagem STDL e o seu motor são propostos para suportar a integração de ambientes heterogéneos de RSSFs e aplicações. As soluções propostas não requerem reprogramação e suportam também serviços de localização nas RSSFs. Diferentes métodos de localização foram avaliados para estimar a localização dos nós. Assim, com estes métodos as RSSFs podem ser usadas como componentes para integrar e suportar a Futura Internet. Todas as soluções propostas nesta tese foram implementadas e validadas tanto em simulação com em plataformas práticas, laboratoriais e industriais. The Wireless Sensor Network (WSN) has a countless number of applications in almost all of the fields including military, industrial, healthcare, and smart home environments. However, there are several problems that prevent the widespread of sensor networks in real situations. Among them, the reliability of communication especially in noisy industrial environments is difficult to guarantee. In addition, interoperability between the sensor networks and external applications is also a challenge. Moreover, determining the position of nodes, particularly mobile nodes, is a critical requirement in many types of applications. My original contributions in this thesis include reliable communication, integration, localization solutions for WSNs operating in industrial and critical environments. Because sensor nodes are usually deployed and kept unattended without human intervention for a long duration, e.g. months or even years, it is a crucial requirement to provide the reliable communication for the WSNs. However, many problems arise during packet transmission and are related to the transmission medium (e.g. signal path-loss, noise and interference). Interference happens due to the existence of more than one network or by the spectral spread that happens in some frequencies. This type of problem is more severe in dynamic environments in which noise sources can be introduced at any time or new networks and devices that interfere with the existing one may be added. Consequently, it is necessary for the WSNs to have the ability to deal with the communication failures. The Dynamic MAC (DynMAC) protocol proposed in this thesis employs the Cognitive Radio (CR) techniques to allow the WSNs to adapt to the dynamic noisy environments by automatically selecting the best channel during its operation time. The WSN usually cannot operate in complete isolation, but it needs to be monitored, controlled and visualized by external applications. Although it is possible to add an IP protocol stack to sensor nodes, this approach is not appropriate for many types of WSNs. Consequently, the proxy and gateway approach is still a preferred method for integrating sensor networks with external networks and applications. The problem of the current integration solutions for WSNs is the adaptability, i.e., the ability of the gateway or proxy developed for one sensor network to be reused, unchanged, for others which have different types of applications and data frames. One reason behind this problem is that it is difficult or even impossible to create a standard for the structure of data inside the frame because there are such a huge number of possible formats. Consequently, it is necessary to have an adaptable solution for easily and transparently integrating WSNs and application environments. In this thesis, the Sensor Traffic Description Language (STDL) was proposed for describing the structure of the sensor networks’ data frames, allowing the framework to be adapted to a diversity of protocols and applications without reprogramming. The positions of sensor nodes are critical in many types of industrial applications such as object tracking, location-aware services, worker or patient tracking, etc. In addition, the sensed data is meaningless without the knowledge of where it is obtained. Perhaps the most well-known location-sensing system is the Global Positioning System (GPS). However, equipping GPS sensor for each sensor node is inefficient or unfeasible for most of the cases because of its energy consumption and cost. In addition, GPS is not appropriate in some environments, e.g., indoors. Similar to the original concept of WSNs, the localization solution should also be cheap and with low power consumption. This thesis aims to deal with the above problems. In particular, in order to add the reliability for WSN, DynMAC protocol was proposed, implemented and evaluated. This protocol adds a mechanism to automatically deal with the noisy and changeable environments. For the second problem, the STDL and its engine provide the adaptable capability to the framework for interoperation between sensor networks and external applications. The proposed framework requires no reprogramming when deploying it for new applications and protocols of WSNs. Moreover, the framework also supports localization services for positioning the unknown position sensor nodes in WSNs. The different localization methods are employed to estimate the location of mobile nodes. With the proposed framework, WSNs can be used as plug and play components for integrating with the Future Internet. All the proposed solutions were implemented and validated using simulation and real testbeds in both the laboratory and industrial environments.
Thesis Type - Doctoral Thesis
URI - http://hdl.handle.net/10316/24532
Title - Proposal and study of an architecture for the management of multiple wireless sensor networks
Author - André Miguel de Almeida Marrão Rodrigues[https://estudogeral.sib.uc.pt/browse?type=author&value=Rodrigues%2C+Andr%C3%A9+Miguel+de+Almeida+Marr%C3%A3o]
Advisor - 
Keywords -  
Date - 11-Jun-2013
Abstract -  
Thesis Type - Doctoral Thesis
URI - http://hdl.handle.net/10316/24528
Title - An environment for services offering in the Internet based on peer-to-peer service overlay networks
Author - Adriano Fiorese[https://estudogeral.sib.uc.pt/browse?type=author&value=Fiorese%2C+Adriano]
Advisor - 
Keywords -  
Date - 11-Jun-2013
Abstract -  
Thesis Type - Doctoral Thesis
URI - http://hdl.handle.net/10316/24526
Title - Proposta e Estudo de Soluções para Otimização de Rotas em Ambientes de Mobilidade de Redes
Author - Pedro Alexandre Vale Pinheiro[https://estudogeral.sib.uc.pt/browse?type=author&value=Pinheiro%2C+Pedro+Alexandre+Vale]
Advisor - 
Keywords -  
Date - 2-Oct-2013
Abstract -  
Thesis Type - Doctoral Thesis
URI - http://hdl.handle.net/10316/24429
Title - Operating Middleware na Timing Guarantees for Heterogeneous Sensor Networks
Author - José Manuel da Silva Cecílio[https://estudogeral.sib.uc.pt/browse?type=author&value=Cec%C3%ADlio%2C+Jos%C3%A9+Manuel+da+Silva]
Advisor - 
Keywords -  
Date - 1-Oct-2013
Abstract -  
Thesis Type - Doctoral Thesis
URI - http://hdl.handle.net/10316/24404
Title - New models to support reliability in Mobile Wireless Sensor Networks
Author - Ricardo Nuno Mendão da Silva[https://estudogeral.sib.uc.pt/browse?type=author&value=Silva%2C+Ricardo+Nuno+Mend%C3%A3o+da]
Advisor - 
Keywords -  
Date - 27-Sep-2013
Abstract -  
Thesis Type - Doctoral Thesis
URI - http://hdl.handle.net/10316/24350
Title - Performance Evaluation and Benchmarking of Event Processing Systems
Author - Marcelo Rodrigues Nunes Mendes[https://estudogeral.sib.uc.pt/browse?type=author&value=Mendes%2C+Marcelo+Rodrigues+Nunes]
Advisor - Pedro Gustavo Santos Rodrigues Bizarro[https://estudogeral.sib.uc.pt/browse?type=author&value=Bizarro%2C+Pedro+Gustavo+Santos+Rodrigues];Paulo Jorge Pimenta Marques[https://estudogeral.sib.uc.pt/browse?type=author&value=Marques%2C+Paulo+Jorge+Pimenta];
Keywords - Desempenho;Sistemas de Processamento de Eventos
Date - 1-Oct-2014
Abstract - Esta dissertação tem por objetivo estudar e comparar o desempenho dos sistemas de processamento de eventos, bem como propor novas técnicas que melhorem sua eficiência e escalabilidade. Nos últimos anos os sistemas de processamento de eventos têm tido uma difusão bastante rápida, tanto no meio acadêmico, onde deram origem a vários projetos de investigação, como na indústria, onde fomentaram o aparecimento de dezenas de startups e fazem-se hoje presentes nos mais diversos domínios de aplicação. No entanto, tem-se observado uma falta generalizada de informação, metodologias de avaliação e ferramentas no que diz respeito ao desempenho das plataformas de processamento de eventos. Até recentemente, não era conhecido ao certo que fatores afetam mais o seu desempenho, se os sistemas seriam capazes de escalar e adaptar-se às mudanças frequentes nas condições de carga, ou se teriam alguma limitação específica. Além disso, a falta de benchmarks padronizados impedia que se estabelecesse qualquer comparação objetiva entre os diversos produtos. Este trabalho visa preencher estas lacunas, e para isso foram abordados quatro tópicos principais. Primeiramente, desenvolvemos o framework FINCoS, um conjunto de ferramentas de benchmarking para a geração de carga e medição de desempenho de sistemas de processamento de eventos. O framework foi especificamente concebido de modo a ser independente dos produtos testados e da carga de trabalho utilizada, permitindo, assim, a sua reutilização em diversos estudos de desempenho e benchmarks. Em seguida, definimos uma série de microbenchmarks e conduzimos um estudo alargado de desempenho envolvendo três sistemas distintos. Essa análise não só permitiu identificar alguns fatores críticos para o desempenho das plataformas de processamento de eventos, como também expôs limitações importantes dos produtos, tais como má utilização de recursos e falhas devido à falta de memória. A partir dos resultados obtidos, passamos a nos dedicar à investigação de melhorias de desempenho. A fim de aprimorar a utilização de recursos, propusemos novos algoritmos e avaliamos esquemas de organização de dados alternativos que não só reduziram substancialmente o consumo de memória, como também se mostraram significativamente mais eficientes ao nível da microarquitetura. Para dirimir o problema de falta de memória, propusemos SlideM, um algoritmo de paginação que seletivamente envia partes do estado de queries contínuas para disco quando a memória física se torna-se insuficiente. Desenvolvemos também uma estratégia baseada no algoritmo SlideM para partilhar recursos computacionais durante o processamento de queries simultâneas. Concluímos esta dissertação propondo o benchmark Pairs. O benchmark visa avaliar a capacidade das plataformas de processamento de eventos em responder rapidamente a números progressivamente maiores de queries e taxas de entrada de dados cada vez mais altas. Para isso, a carga de trabalho do benchmark foi cuidadosamente concebida de modo a exercitar as operações encontradas com maior frequência em aplicações reais de processamento de eventos, tais como agregação, correlação e detecção de padrões. O benchmark Pairs também se diferencia de propostas anteriores em áreas relacionadas por permitir avaliar outros aspectos fundamentais, como adaptabilidade e escalabilidade com relação ao número de queries. De uma forma geral, esperamos que os resultados e propostas apresentados neste trabalho venham a contribuir para ampliar o entendimento acerca do desempenho das plataformas de processamento de eventos, e sirvam como estímulo para novos projetos de investigação que levem a melhorias adicionais à geração atual de sistemas. This thesis aims at studying, comparing, and improving the performance and scalability of event processing (EP) systems. In the last 15 years, event processing systems have gained increased attention from academia and industry, having found application in a number of mission-critical scenarios and motivated the onset of several research projects and specialized startups. Nonetheless, there has been a general lack of information, evaluation methodologies and tools in what concerns the performance of EP platforms. Until recently, it was not clear which factors impact most their performance, if the systems would scale well and adapt to changes in load conditions or if they had any serious limitations. Moreover, the lack of standardized benchmarks hindered any objective comparison among the diverse platforms. In this thesis, we tackle these problems by acting in several fronts. First, we developed FINCoS, a set of benchmarking tools for load generation and performance measurement of event processing systems. The framework has been designed to be independent on any particular workload or product so that it can be reused in multiple performance studies and benchmark kits. FINCoS has been made publicly available under the terms of the GNU General Public License and is also currently hosted at the Standard Performance Evaluation Corporation (SPEC) repository of peer-reviewed tools for quantitative system evaluation and analysis. We then defined a set of microbenchmarks and used them to conduct an extensive performance study on three EP systems. This analysis helped identifying critical factors affecting the performance of event processing platforms and exposed important limitations of the products, such as poor utilization of resources, trashing or failures in the presence of memory shortages, and no/incipient query plan sharing capabilities. With these results in hands, we moved our focus to performance enhancement. To improve resource utilization, we proposed novel algorithms and evaluated alternative data organization schemes that not only reduce substantially memory consumption, but also are significantly more efficient at the microarchitectural level. Our experimental evaluation corroborated the efficacy of the proposed optimizations: together they provided a 6-fold reduction in memory usage and order-of-magnitude increase on query throughput. In addition, we addressed the problem of memory-constrained applications by introducing SlideM, an optimal buffer management algorithm that selectively offloads sliding windows state to disk when main memory becomes insufficient. We also developed a strategy based on SlideM to share computational resources when processing multiple aggregation queries over overlapping sliding windows. Our experimental results demonstrate that, contrary to common sense, storing windows data on disk can be appropriate even for applications with very high event arrival rates. We concluded this thesis by proposing the Pairs benchmark. Pairs was designed to assess the ability of EP platforms in processing increasingly larger numbers of simultaneous queries and event arrival rates while providing quick answers. The benchmark workload exercises several common features that appear repeatedly in most event processing applications, including event filtering, aggregation, correlation and pattern detection. Furthermore, differently from previous proposals in related areas, Pairs allows evaluating important aspects of event processing systems such as adaptivity and query scalability. In general, we expect that the findings and proposals presented in this thesis serve to broaden the understanding on the performance of event processing platforms and open avenues for additional improvements in the current generation of EP systems.
Thesis Type - Doctoral Thesis
URI - http://hdl.handle.net/10316/24296
Title - Contributions to Electrical Energy Disaggregation in a Smart Home
Author - Marisa Figueiredo[https://estudogeral.sib.uc.pt/browse?type=author&value=Figueiredo%2C+Marisa]
Advisor - Bernardete Ribeiro[https://estudogeral.sib.uc.pt/browse?type=author&value=Ribeiro%2C+Bernardete];Ana de Almeida[https://estudogeral.sib.uc.pt/browse?type=author&value=Almeida%2C+Ana+de];
Keywords - Desagregação de sinal eléctrico;Separação de fontes;Extracção de caracteristicas e classificação de padrões
Date - 7-Mar-2014
Abstract - A eficiência energética pode ser o futuro em termos de recursos energéticos. Nos últimos anos, a consciencialização da sociedade relativamente às mudanças ambientais e aos elevados custos energéticos tem aumentado. Contudo, o uso desajustado dos aparelhos eléctricos ainda representa uma fatia substancial do consumo de energia. A monitorização contínua e detalhada, como já demonstrado, é essencial para assegurar a eficiência energética em edifícios como as nossas casas. O consumo detalhado por electrodoméstico é uma mais-valia para o consumidor, conduzindo-o a escolhas informadas e mudanças de comportamentos. Sistemas de monitorização não intrusiva de cargas (NILM) possibilitam a monitorização da energia, a previsão dos consumos e o controlo dos aparelhos eléctricos residenciais, constituindo uma solução atractiva para munir o consumidor final com o gasto detalhado por equipamento. Acedendo apenas aos dados relativos ao consumo agregado de electricidade, adquiridos num único ponto (quadro eléctrico geral), estes sistemas discriminam os gastos de cada aparelho através de algoritmos de aprendizagem computacional e de reconhecimento de padrões. Considerando o baixo custo associado, a instalação fácil e o potencial associado às redes energéticas inteligentes do futuro, pois facilitará a participação dos consumidores finais no mercado da electricidade, o NILM tornou-se uma área activa de investigação. A presente Tese foca-se na desagregação de energia, enquadrado num sistema NILM. Assentando na disponibilidade dos dados de consumo agregado, esta Tese tem por objectivo investigar e explorar metodologias ainda não aplicadas à resolução do problema de separação do referido consumo nos gastos dos diversos equipamentos, ou grupos, ligados ao circuito eléctrico da casa.Uma abordagem comum considera a questão da desagregação como um problema de classificação, requerendo a definição de assinaturas dos vários equipamentos. Todavia, ainda não foi encontrado um conjunto de características distintivas apto a descrever com precisão cada aparelho. Assim sendo, esta Tese procura reforçar a procura de um tal conjunto de características. Para tal, é introduzida uma regra para a identificação de estados estáveis, bem como a sua demonstração matemática, utilizada para o reconhecimento de step-changes nos sinais de potência activa e reactiva e factor potência. As step-changes identificadas definem as assinaturas dos equipamentos posteriormente utilizadas por dois métodos de classificação para o reconhecimento dos electrodomésticos, a saber: 5-Vizinhos Mais Próximos e Máquinas de Vectores de Suporte. As experiências computacionais, baseadas em dados reais, mostraram a eficácia da assinatura proposta para a distinção das diferentes cargas em estudo. A desagregação e extracção de informação relevante dos dados agregados de consumo eléctrico podem ser interpretadas à luz da área de processamento de sinal. Neste sentido, as abordagens de análise de sinal e de séries temporais adequam-se à extracção de informação do consumo agregado. Com efeito, previamente ao cálculo das estimativas dos gastos de cada um dos aparelhos, o estudo foca-se na extracção de variações contidas no sinal agregado e associadas a equipamentos cuja operação não requer qualquer tipo de interacção humana. Neste contexto, é proposto um método que tem por base a técnica de Wavelet Shrinkage em conjunto com operações de processamento de sinais para a extracção de informação do sinal agregado. Note-se que o referido método proposto assenta no pressuposto de que vários segmentos podem ser analisados por funções wavelet distintas. No mesmo contexto, a desagregação de energia pode ser interpretada como um problema de separação de fontes quando apenas um sinal de mistura é conhecido. Assim sendo, foi analisado o desempenho da modelação de fontes com recurso a vectores multidimensionais e correspondente método de factorização. Assumindo que um vector multidimensional composto pelos dados de gastos dos vários equipamentos da casa pode ser definido, a sua factorização não-negativa é executada de forma a extrair os componentes mais relevantes. Os factores resultantes são incorporados no processo de inferência das fontes, no qual apenas o consumo agregado da casa está disponível. As estimativas dos consumos associados a cada equipamento são, então, obtidas pela factorização não-negativa de matrizes. Tanto a abordagem para a extracção de variações como o método proposto para a desagregação de sinal foram avaliados com sucesso num conjunto de dados real, para o qual um desempenho favorável foi observado e validado através de análise estatística. Em suma, esta Tese contribui com abordagens para a desagregação de energia eléctrica, testadas e validadas em dados reais, pelo que esperamos que venha a ter um impacto tangível na resolução de problemas de eficiência numa casa inteligente. Tomorrow's main energy resource may well be energy efficiency. Over the last years, society awareness on environmental changes and high energy costs has been increasing. Nevertheless, the improper use of electrical devices still represents a substantial slice of the electrical energy consumption. Continuous and detailed electricity monitoring has been demonstrated an essential tool to ensure energy efficient in buildings as our homes. Appliance-specific consumption information empowers consumers, leading to informed choices and change of behaviours. Non-intrusive Load Monitoring (NILM) systems, aiming at energy monitoring, load forecasting and improved control of residential appliances, are an attractive solution to bring detailed consumption at device-level to end-users. Using only the aggregated electricity consumption data acquired at a single-point, usually the utility-customer interface, NILM discerns appliances' power usage data employing machine learning and pattern recognition algorithms. Due to its possible low cost, easy installation and easy integration into the future smart grids, which would enable consumers to participate in the electricity market, NILM has become an active area of research. This Thesis is concerned with energy disaggregation as the key part of a NILM framework. Given the whole-home electrical consumption data it aims at investigating and exploring methodologies not yet applied to tackle the correct disaggregation of this signal into the detailed usage of each appliance, or groups of devices, connected to the home electrical circuit. Widespread NILM approaches usually explore the disaggregation of single-point acquired data as a classification problem for which appliances signatures are required. Yet, no set of distinctive characteristics able to accurately describe each appliance has been found. Thereby, this thesis reinforces the search for the set of features used as appliances signatures. Namely, a rule for steady-state identification and its mathematical proof are introduced. This rule was applied for detection of step-changes occurring in the active and reactive power signals and the power factor measurements. The step-changes identified comprised a new appliance signature posteriorly used by the 5-Nearest Neighbours and the Support Vector Machines classification methods in order to obtain the appliance identification. The computational experiments yielded in real-world dataset showed the effectiveness of the proposed signature for distinguishing the different loads in study. The disaggregation and extraction of meaningful information from the aggregated electricity consumption can alternatively be interpreted in the light of signal processing analysis. In this sense, signal processing and time series analysis strategies arise as suitable tools for the extraction of information from the whole-home signal. Before aiming at the calculation of consumption estimates for each appliance, a previous study concerning the extraction of variations in the aggregated electrical signal associated with devices that work automatically without any human intervention is performed. In this context, a technique based on Wavelet Shrinkage and signal processing operations, designed to extract information from the aggregated signal considering several of its segments that can be analysed by distinct mother wavelets, is proposed. Following this path, a novel way to look into the issue of energy disaggregation is its interpretation as a single-channel source separation problem. To this end, the performance of source modelling based on multi-way arrays (tensors) and correspondent factorization is analysed. With the proviso that a tensor composed by the data for the several devices in the house is given, non-negative tensor factorization is performed in order to extract the most relevant components. The outcome is later embedded in the test step, where only the whole-home measured consumption is available. Inference of individual consumptions is then achieved by matrix factorization using the learned models. The approaches based on signal processing, for the extraction and disaggregation of information from the whole-home electrical signal, were successfully evaluated on a real-world dataset, as illustrated by the favourable performance and statistical evidence. Overall, this Thesis contributes with electrical energy disaggregation approaches, successfully validated on real-world data, which - as we hope - will have a positive impact in solving efficiency problems in a smart home.
Thesis Type - Doctoral Thesis
URI - http://hdl.handle.net/10316/24256
Title - Dependability Benchmarking for Large and Complex Systems
Author - Pedro Miguel Lopes Nunes da Costa[https://estudogeral.sib.uc.pt/browse?type=author&value=Costa%2C+Pedro+Miguel+Lopes+Nunes+da]
Advisor - João Gabriel Monteiro de Carvalho e Silva[https://estudogeral.sib.uc.pt/browse?type=author&value=Silva%2C+Jo%C3%A3o+Gabriel+Monteiro+de+Carvalho+e];Henrique Santos do Carmo Madeira[https://estudogeral.sib.uc.pt/browse?type=author&value=Madeira%2C+Henrique+Santos+do+Carmo];
Keywords - Dependability Benchmarking;Software Faults;Software Implemented Fault Injection
Date - 13-Dec-2013
Abstract - The spread of computer-based systems and the growing number of its applications in critical tasks has increased the dependence of modern societies on that kind of systems. As a consequence, dependability benchmarking of computer systems, as a way to assess and compare the dependability of components and systems, has caught the attention of researchers and practitioners in recent years. One crucial component of dependability benchmarks is the fault injector. Dependability benchmarks must include fault injectors with very specific features: (i) they should be very easy to install and use, without the need for any complex setup or installation procedure;(ii) have high level of portability; (iii) have very low intrusiveness, in order to mitigate the performance loss; (iv) be capable of injecting faults in both user and system spaces; (v) and in code and data segments of any process, irrespective of their complexity; (vi) be independent of the availability of the source code of any system component or user process; (vii) be dynamically linked into a target system; and (viii) be compatible with the latest and most advanced software fault models. Since existing fault injectors do not fulfill these requirements, this thesis presents a pioneering SWIFI tool named DBench FI (Dependability Benchmarking Fault Injector), specially developed for dependability benchmarking. Their unique characteristics make it one of the most versatile fault injectors available. Among the main components of a dependability benchmark suite, the most critical one is undoubtedly the faultload. It should embody a repeatable, portable, representative and generally accepted fault set. Concerning software faults, the definition of that kind of faultloads is particularly difficult, as it requires a much more complex emulation method than the traditional stuck-at or bit-flip used for hardware faults. Moreover, a faultload based on software faults requires a clear separation between the software components which are selected as fault injection target and the benchmark target (i.e., the system under evaluation), as the injection of software faults actually changes the code of the target component. This way, the faults should be injected in one component (the fault injection target) in order to evaluate their impact in the other components or in the overall system, guaranteeing the inviolability of the benchmark target and the credibility of the dependability benchmark. Although faultloads based on software faults had already been proposed, the choice of adequate fault injection targets (i.e., actual software components where the faults are injected) is still an open and crucial issue. Knowing that the number of possible software faults that can be injected in a given system is potentially very large (especially for large and complex systems), the problem of defining a faultload made of a small number of representative faults is of utmost importance. This thesis presents a comprehensive fault injection study and proposes a strategy to guide the fault injection target selection to reduce the number of faults required for the faultload. Furthermore, it exemplifies the proposed approach with a real web-server dependability benchmark and a large-scale integer vector sort application. O aumento da utilização dos sistemas informáticos e o número crescente das suas aplicações em tarefas críticas das sociedades modernas tem aumentado a dependência desse tipo de sistemas. Em consequência, nos últimos anos, as benchmarks de confiabilidade têm sido objeto de enorme interesse, quer por parte de investigadores, quer por parte da indústria. Um dos elementos fundamentais que integram as benchmarks de confiabilidade é o injetor de falhas. As benchmarks de confiabilidade devem incluir injetores de falhas com características muito específicas: (i) devem ser fáceis de instalar e de utilizar, não exigindo qualquer procedimento especial de instalação ou execução; (ii) devem possuir um elevado nível de portabilidade; (iii) devem possuir um baixo nível de intrusividade no sistema alvo, de forma a minorar a perda de desempenho; (iv) devem oferecer a capacidade de injetar falhas em todo o sistema alvo (quer no espaço do utilizador, quer no espaço do sistema); (v) assim como nos segmentos de código e de dados de qualquer processo, independentemente da sua complexidade; (vi) devem ser independentes da disponibilidade ou conhecimento do código fonte de qualquer componente do sistema ou processo de utilizador; (vii) ser dinamicamente integrados no sistema alvo; e (viii) ser compatíveis com os mais avançados e recentes modelos de falhas de software. Uma vez que os atuais injetores de falhas não satisfazem todos os requisitos mencionados, esta tese apresenta uma ferramenta de injeção de falhas pioneira, implementada por software (Software Implemented Fault Injection SWIFI), denominada DBench-FI, especialmente desenvolvida para benchmarks de confiabilidade. As suas características únicas fazem dele um dos mais versáteis injetores de falhas atualmente existentes. De entre os componentes fundamentais das benchmarks de confiabilidade (workload, faultload, medidas, e configuração experimental e procedimentos), a faultload é, sem dúvida, um dos mais críticos. Ela deve incorporar um conjunto de falhas repetível, portável, representativo e aceite pela comunidade e pela indústria. No que concerne a falhas de software, a definição desse tipo de fautloads é particularmente difícil, uma vez que exige métodos bastante mais complexos do que o tradicional stuck at ou bit-flip utilizado nas falhas de hardware. Adicionalmente, as faultload baseadas em falhas de software exigem uma clara separação entre os componentes de software que são selecionados como alvo da injeção de falhas e o alvo da benchmark (i.e., o sistema sob avaliação), uma vez que a injeção de falhas de software altera efetivamente o código do componente alvo. Desta forma, as falhas devem ser injetadas num componente (o alvo da injeção de falhas) a fim de se avaliar o seu impacto nos outros componentes ou no sistema como um todo, garantindo a inviolabilidade do alvo da benchmark e a credibilidade das benchmarks de confiabilidade. Apesar de terem já sido propostas faultloads baseadas em falhas de software, a escolha dos alvos da injeção de falhas (ou seja, os componentes de software onde as falhas são injetadas) continua a ser um tópico em aberto, apesar de fundamental. Sabendo-se que o número de falhas de software que podem ser injetadas num dado sistema é potencialmente muito grande, o problema da definição de uma faultload composta por um número pequeno de falhas representativas é de extrema importância. Esta tese apresenta igualmente um estudo exaustivo de injeção de falhas e propõe uma estratégia de orientação da seleção dos alvos da injeção de falhas para a redução o número de falhas necessárias numa faultload. Além disso, exemplifica a abordagem proposta com a utilização de uma benchmark de confiabilidade, real, para web-servers e de uma aplicação de ordenação de vetores de números inteiros de larga dimensão.
Thesis Type - Doctoral Thesis
URI - http://hdl.handle.net/10316/24220
